{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62aa155e-9613-4ca9-a88d-d11ce4ac8b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA H100 80GB HBM3 \n",
      "GPU 1: NVIDIA H100 80GB HBM3 \n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import contextlib\n",
    "import os\n",
    "import signal\n",
    "import subprocess\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import requests\n",
    "import transformers\n",
    "from transformers.utils import logging\n",
    "\n",
    "from nemo_skills.code_execution.sandbox import get_sandbox\n",
    "from nemo_skills.inference.model import get_code_execution_model\n",
    "from nemo_skills.prompt.utils import get_prompt\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "!nvidia-smi -L | cut -d '(' -f 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed515c34-98c2-4d46-b00a-239a3faa47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"./\"\n",
    "MODEL_DIR_HF = f\"{BASE_DIR}/OpenMath-Nemotron-14B-kaggle\"\n",
    "MODEL_DIR_BF16 = f\"{BASE_DIR}/OpenMath-Nemotron-14B-kaggle-bf16-trtllm\"\n",
    "MODEL_DIR_FP8 = f\"{BASE_DIR}/OpenMath-Nemotron-14B-kaggle-fp8-trtllm\"\n",
    "MODEL_DIR_FP8_DRAFT = f\"{BASE_DIR}/OpenMath-Nemotron-14B-kaggle-fp8-redrafter-trtllm\"\n",
    "benchmark = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd2af26-291d-4d2a-a3f8-07b98354e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_server(host, port, timeout=300, interval=1):\n",
    "    url = f\"http://{host}:{port}\"\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.put(url)\n",
    "            if response.status_code != 403:\n",
    "                return True\n",
    "        except requests.RequestException:\n",
    "            if time.time() - start_time > timeout:\n",
    "                raise TimeoutError(\"Server did not respond within timeout period\")\n",
    "            time.sleep(interval)\n",
    "\n",
    "\n",
    "def start_server(model_dir, port=5000):\n",
    "    host = \"127.0.0.1\"\n",
    "    cmd = (\n",
    "        f\"trtllm-serve serve {model_dir} \"\n",
    "        f\"    --tokenizer {MODEL_DIR_HF}\"\n",
    "        f\"    --backend trt \"\n",
    "        f\"    --tp_size 2 \"\n",
    "        f\"    --kv_cache_free_gpu_memory_fraction 0.92 \"\n",
    "        f\"    --max_batch_size 12 \"\n",
    "        f\"    --host {host} \"\n",
    "        f\"    --port {port}\"\n",
    "    )\n",
    "    print(f\"Starting server from {model_dir} at {host}:{port}\")\n",
    "    model_name = model_dir.split(\"/\")[-1]\n",
    "    log_path = Path(f\"{model_name}_server_logs.log\").resolve()\n",
    "    log_file = open(log_path, \"w\", buffering=1)\n",
    "    proc = subprocess.Popen(cmd, shell=True, stdout=log_file, stderr=subprocess.STDOUT, preexec_fn=os.setsid)\n",
    "    print(\"Waiting for server to be ready (might take a while) ...\")\n",
    "    wait_for_server(host, port)\n",
    "    print(\"Server ready!\")\n",
    "    return proc\n",
    "\n",
    "\n",
    "def kill_server(proc, port=5000):\n",
    "    os.killpg(proc.pid, signal.SIGTERM)\n",
    "    time.sleep(10)\n",
    "\n",
    "    for proc in psutil.process_iter([\"pid\", \"name\"]):\n",
    "        for conn in proc.connections(kind=\"inet\"):\n",
    "            if conn.laddr.port == port:\n",
    "                print(f\"Killing process {proc.info['name']} (PID: {proc.info['pid']}) running on port {port}\")\n",
    "                os.kill(proc.info[\"pid\"], 9)\n",
    "                break\n",
    "\n",
    "    time.sleep(10)\n",
    "    print(\"Server closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93801d3c-3388-4d2d-982a-031b39d0bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_generation(request_id, prompt_obj, llm, problem):\n",
    "    stream = None\n",
    "    try:\n",
    "        stream = await llm.generate_async(\n",
    "            prompt=prompt_obj.fill({\"problem\": problem}),\n",
    "            stream=True,\n",
    "            random_seed=request_id,\n",
    "            temperature=0.7,\n",
    "            tokens_to_generate=20000,\n",
    "            **prompt_obj.get_code_execution_args(),\n",
    "        )\n",
    "        full_generation = \"\"\n",
    "        async for response in stream:\n",
    "            full_generation += response[\"generation\"]\n",
    "        return full_generation\n",
    "    except asyncio.CancelledError:\n",
    "        # Close the stream so the server is notified on cancellation\n",
    "        with contextlib.suppress(Exception):\n",
    "            if stream is not None:\n",
    "                if hasattr(stream, \"aclose\"):\n",
    "                    await stream.aclose()\n",
    "                elif hasattr(stream, \"close\"):\n",
    "                    stream.close()\n",
    "        raise\n",
    "\n",
    "\n",
    "async def main_loop(prompt_obj, llm, problem):\n",
    "    num_generations = 12\n",
    "    cancel_after_done = 10\n",
    "    tasks = [asyncio.create_task(run_generation(i, prompt_obj, llm, problem)) for i in range(num_generations)]\n",
    "\n",
    "    completed = 0\n",
    "    all_generations = []\n",
    "\n",
    "    # Consume as they finish\n",
    "    for fut in asyncio.as_completed(tasks):\n",
    "        res = await fut\n",
    "        all_generations.append(res)\n",
    "        completed += 1\n",
    "        if completed >= cancel_after_done:\n",
    "            # cancel remaining\n",
    "            for t in tasks:\n",
    "                if not t.done():\n",
    "                    t.cancel()\n",
    "            break\n",
    "\n",
    "    # Drain only the still-pending tasks to avoid duplicates\n",
    "    pending = [t for t in tasks if not t.done()]\n",
    "    if pending:\n",
    "        drained = await asyncio.gather(*pending, return_exceptions=True)\n",
    "        # keep only successful returns (skip exceptions like CancelledError)\n",
    "        all_generations.extend(r for r in drained if not isinstance(r, Exception))\n",
    "\n",
    "    return all_generations\n",
    "\n",
    "\n",
    "def build_table(bench, tokenizer):\n",
    "    data = {\n",
    "        \"Metric\": [\n",
    "            \"Num Generations\",\n",
    "            \"Total Generation Time\",\n",
    "            \"Batch Throughput (Tok/sec)\",\n",
    "            \"Avg Request Throughput (Tok/sec)\",\n",
    "        ]\n",
    "    }\n",
    "    for name, rec in bench.items():\n",
    "        gens = [g for g in rec[\"gens\"] if isinstance(g, str)]\n",
    "        tt = max(rec[\"total_time\"], 1e-9)\n",
    "        toks = np.array([len(tokenizer.encode(g)) for g in gens], dtype=float)\n",
    "        batch_tp = toks.sum() / tt\n",
    "        per_req_tp = toks / tt\n",
    "        data[name] = [\n",
    "            f\"{len(gens)}\",\n",
    "            f\"{tt:.1f}\",\n",
    "            f\"{batch_tp:.1f}\",\n",
    "            f\"{per_req_tp.mean():.1f} Â± {per_req_tp.std():.1f}\",\n",
    "        ]\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97195ac8-a7ec-4cc5-8e4e-91d98ffba54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[worker unknown] 2025-08-29 09:23:53,871 INFO: Applied worker memory limit (RLIMIT_AS/RLIMIT_DATA): 21474836480 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'local_sandbox_server'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "sandbox_cmd = \"python -m nemo_skills.code_execution.local_sandbox.local_sandbox_server\"\n",
    "subprocess.Popen(sandbox_cmd, shell=True)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "627617b1-edad-47ec-8237-b9276fca85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sandbox = get_sandbox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4744baf-99be-48f8-bb1f-4d9513e208cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server from .//OpenMath-Nemotron-14B-kaggle-fp8-redrafter-trtllm at 127.0.0.1:5000\n",
      "Waiting for server to be ready (might take a while) ...\n",
      "Server ready!\n"
     ]
    }
   ],
   "source": [
    "server_process = start_server(MODEL_DIR_FP8_DRAFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5764571b-fdae-451a-b756-a6fbf0c97d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"OpenMath-Nemotron-14B-kaggle-fp8-redrafter-trtllm\",\"object\":\"model\",\"created\":1756459594,\"owned_by\":\"tensorrt_llm\"}]}"
     ]
    }
   ],
   "source": [
    "# Openai trtllm server\n",
    "!curl -s http://127.0.0.1:5000/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f197f46-a96f-44af-98b4-cb1206d7af34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"backend\":\"ipython\",\"sessions\":{}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[worker unknown] 2025-08-29 09:26:34,667 INFO: active_sessions=0\n"
     ]
    }
   ],
   "source": [
    "# Code execution server\n",
    "!curl -s http://127.0.0.1:6000/sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfd2db6b-33b1-4ace-acc6-b219aa9dbdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_DIR_FP8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cdc825f-d200-48d6-8d61-ae6d83a7a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox = get_sandbox()\n",
    "code_execution = {\"max_code_executions\": 2, \"sandbox_traceback_verbosity\": \"plain\"}\n",
    "prompt_obj = get_prompt(\"generic/math\", tokenizer=MODEL_DIR_FP8, code_tags=\"openmath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d9d9651-4da8-4ea5-9817-0cf1cba21b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_code_execution_model(\n",
    "    server_type=\"trtllm\", model=MODEL_DIR_FP8, sandbox=sandbox, code_execution=code_execution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5765fca5-84d5-4748-b7c9-12a4e59354a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = r\"\"\"\n",
    "The Fibonacci numbers are defined as follows: $F_0 = 0$, $F_1 = 1$, and $F_{n+1} = F_n + F_{n-1}$ for $n \\geq 1$.\n",
    "There are $N$ positive integers $n$ strictly less than $10^{101}$ such that $n^2 + (n+1)^2$ is a multiple of 5 but $F_{n-1}^2 + F_n^2$ is not.\n",
    "How many prime factors does $N$ have, counted with multiplicity?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e69a26-bd9e-4a46-b03c-c73463314f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad8a8b07-35bf-4a70-a8e6-66a9f21193cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df130bdc-d05d-4264-8d28-a44cb6c9e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter()\n",
    "gens_fp8_draft = await main_loop(prompt_obj, llm, problem)\n",
    "bench[\"fp8_draft\"] = {\n",
    "    \"gens\": gens_fp8_draft,\n",
    "    \"total_time\": time.perf_counter() - t0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9603c44b-934f-453d-b5a6-8bbdadbcb5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server closed.\n"
     ]
    }
   ],
   "source": [
    "kill_server(server_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1f8515c-59d1-40e1-a44c-be7e5aa5dc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server from .//OpenMath-Nemotron-14B-kaggle-fp8-trtllm at 127.0.0.1:5000\n",
      "Waiting for server to be ready (might take a while) ...\n",
      "Server ready!\n"
     ]
    }
   ],
   "source": [
    "server_process = start_server(MODEL_DIR_FP8)\n",
    "llm = get_code_execution_model(\n",
    "    server_type=\"trtllm\", model=MODEL_DIR_FP8, sandbox=sandbox, code_execution=code_execution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7440221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter()\n",
    "gens_fp8 = await main_loop(prompt_obj, llm, problem)\n",
    "bench[\"fp8\"] = {\n",
    "    \"gens\": gens_fp8,\n",
    "    \"total_time\": time.perf_counter() - t0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e458122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server closed.\n"
     ]
    }
   ],
   "source": [
    "kill_server(server_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b72e2cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server from .//OpenMath-Nemotron-14B-kaggle-bf16-trtllm at 127.0.0.1:5000\n",
      "Waiting for server to be ready (might take a while) ...\n",
      "Server ready!\n"
     ]
    }
   ],
   "source": [
    "server_process = start_server(MODEL_DIR_BF16)\n",
    "llm = get_code_execution_model(\n",
    "    server_type=\"trtllm\", model=MODEL_DIR_BF16, sandbox=sandbox, code_execution=code_execution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16d33377",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter()\n",
    "gens_bf16 = await main_loop(prompt_obj, llm, problem)\n",
    "bench[\"bf16\"] = {\n",
    "    \"gens\": gens_bf16,\n",
    "    \"total_time\": time.perf_counter() - t0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a909688-bc2f-4b95-ae59-ce0986e1c9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>fp8_draft</th>\n",
       "      <th>fp8</th>\n",
       "      <th>bf16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Num Generations</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Total Generation Time</td>\n",
       "      <td>30.5</td>\n",
       "      <td>64.7</td>\n",
       "      <td>144.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Batch Throughput (Tok/sec)</td>\n",
       "      <td>1385.4</td>\n",
       "      <td>751.7</td>\n",
       "      <td>346.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avg Request Throughput (Tok/sec)</td>\n",
       "      <td>138.5 Â± 24.6</td>\n",
       "      <td>75.2 Â± 11.0</td>\n",
       "      <td>34.6 Â± 6.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Metric     fp8_draft          fp8        bf16\n",
       "0                   Num Generations            10           10          10\n",
       "1             Total Generation Time          30.5         64.7       144.2\n",
       "2        Batch Throughput (Tok/sec)        1385.4        751.7       346.0\n",
       "3  Avg Request Throughput (Tok/sec)  138.5 Â± 24.6  75.2 Â± 11.0  34.6 Â± 6.1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_table(bench, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da901d45-bb3a-42e2-9b1d-41f2fee11757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
