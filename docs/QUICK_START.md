# ğŸš€ å¿«é€Ÿå¼€å§‹ - è¯„ä¼°ç®¡çº¿

## ğŸ“¦ å·²æœ‰æ•°æ®é›†ï¼Œç›´æ¥è¿è¡Œ

### LAMBADA è¯„ä¼°
```bash
python run_pipeline.py \
    --dataset lambada \
    --dataset-path /home/rwkv/Peter/Albatross/eval/lambada_test.jsonl \
    --evaluator perplexity \
    --batch-size 256
```

### æŸ¥çœ‹å¯ç”¨ç»„ä»¶
```bash
python run_pipeline.py --list
```

### å¿«é€Ÿæµ‹è¯•ï¼ˆé™åˆ¶æ ·æœ¬æ•°ï¼‰
```bash
python run_pipeline.py \
    --dataset lambada \
    --dataset-path /path/to/lambada.jsonl \
    --evaluator perplexity \
    --limit 100  # åªè·‘100ä¸ªæ ·æœ¬
```

---

## â• æ·»åŠ æ–°æ•°æ®é›†ï¼ˆä»¥ MMLU Pro ä¸ºä¾‹ï¼‰

### ç¬¬1æ­¥ï¼šåˆ›å»ºæ³¨å†Œæ–‡ä»¶ `register_mmlu_pro.py`

```python
from dataset_registry import DatasetRegistry
from evaluator_registry import EvaluatorRegistry
import json

# 1ï¸âƒ£ æ•°æ®åŠ è½½å™¨
def load_mmlu_pro(path: str):
    with open(path, 'r') as f:
        return [json.loads(line) for line in f]

# 2ï¸âƒ£ Prompt Template
def mmlu_pro_prompt(item: dict) -> str:
    question = item['question']
    options = item['options']
    
    option_text = '\n'.join([
        f"{chr(65+i)}. {opt}" 
        for i, opt in enumerate(options)
    ])
    
    return f"Question: {question}\n\n{option_text}\n\nAnswer:"

# 3ï¸âƒ£ è¯„ä¼°å™¨ï¼ˆå¯é€‰ï¼Œå¯ä»¥ç”¨å†…ç½®çš„ï¼‰
def mmlu_pro_evaluator(engine, data, batch_size=64, **kwargs):
    correct = 0
    for i in range(0, len(data), batch_size):
        batch = data[i:i+batch_size]
        prompts = [item['prompt'] for item in batch]
        
        tokens, _ = engine.generate_batch(prompts, max_length=10)
        predictions = engine.decode_tokens(tokens)
        
        for pred, item in zip(predictions, batch):
            pred_answer = pred.strip().upper()[0] if pred else ''
            if pred_answer == item['reference']:
                correct += 1
    
    return {'accuracy': correct / len(data)}

# 4ï¸âƒ£ æ³¨å†Œ
DatasetRegistry.register(
    name='mmlu_pro',
    loader=load_mmlu_pro,
    prompt_template=mmlu_pro_prompt,
    description='MMLU Pro',
    default_batch_size=64,
    default_max_length=10
)

EvaluatorRegistry.register(
    name='mmlu_pro',
    evaluator=mmlu_pro_evaluator,
    description='Multi-choice accuracy',
    metrics=['accuracy']
)

print("âœ“ MMLU Pro registered!")
```

### ç¬¬2æ­¥ï¼šä½¿ç”¨

**å‘½ä»¤è¡Œï¼š**
```bash
python run_pipeline.py \
    --dataset mmlu_pro \
    --dataset-path /path/to/mmlu_pro.jsonl \
    --evaluator mmlu_pro
```

**Python ä»£ç ï¼š**
```python
from register_mmlu_pro import *  # è‡ªåŠ¨æ³¨å†Œ
from pipeline import quick_eval

results = quick_eval(
    model_path="/home/rwkv/models/rwkv7/rwkv7-g1-0.4b-20250324-ctx4096",
    dataset_name="mmlu_pro",
    dataset_path="/path/to/mmlu_pro.jsonl",
    evaluator_name="mmlu_pro"
)
```

---

## ğŸ¨ å¸¸è§ Prompt æ¨¡æ¿

### ç®€å• QA
```python
def qa_prompt(item):
    return f"Q: {item['question']}\nA:"
```

### å¤šé€‰é¢˜
```python
def mcq_prompt(item):
    opts = '\n'.join([f"{chr(65+i)}. {o}" for i, o in enumerate(item['options'])])
    return f"{item['question']}\n\n{opts}\n\nAnswer:"
```

### Chain-of-Thought
```python
def cot_prompt(item):
    return f"{item['question']}\n\nLet's think step by step:\n"
```

### æŒ‡ä»¤è·Ÿéš
```python
def instruction_prompt(item):
    return f"{item['instruction']}\n\nInput: {item['input']}\nOutput:"
```

---

## ğŸ“ æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | ç”¨é€” |
|------|------|
| `dataset_registry.py` | æ•°æ®é›†æ³¨å†Œå™¨ï¼ˆåŠ è½½+promptæ¨¡æ¿ï¼‰ |
| `evaluator_registry.py` | è¯„ä¼°å™¨æ³¨å†Œå™¨ï¼ˆæŒ‡æ ‡è®¡ç®—ï¼‰ |
| `pipeline.py` | è¯„ä¼°ç®¡çº¿æ ¸å¿ƒ |
| `run_pipeline.py` | å‘½ä»¤è¡Œå…¥å£ |
| `batch_engine.py` | RWKV æ¨ç†å¼•æ“ |
| `register_*.py` | å„æ•°æ®é›†æ³¨å†Œæ–‡ä»¶ |

---

## ğŸ”§ å†…ç½®ç»„ä»¶

### å†…ç½®æ•°æ®é›†
- `lambada` - LAMBADA è¯­è¨€å»ºæ¨¡
- `qa` - é€šç”¨é—®ç­”
- `completion` - æ–‡æœ¬è¡¥å…¨
- `cot` - Chain-of-Thought
- `math` - æ•°å­¦é—®é¢˜

### å†…ç½®è¯„ä¼°å™¨
- `perplexity` - å›°æƒ‘åº¦ + å‡†ç¡®ç‡ï¼ˆè¯­è¨€å»ºæ¨¡ï¼‰
- `exact_match` - ç²¾ç¡®åŒ¹é…ï¼ˆåˆ†ç±»ã€QAï¼‰
- `generation` - ç”Ÿæˆç»Ÿè®¡ï¼ˆthroughputã€tokensï¼‰

---

## ğŸ“Š ç»“æœè¾“å‡º

è¯„ä¼°ç»“æœä¿å­˜åœ¨ `./eval_results/` ç›®å½•ï¼š

```
eval_results/
â”œâ”€â”€ lambada_perplexity_20241016_123456.json  # å•ä»»åŠ¡ç»“æœ
â”œâ”€â”€ mmlu_pro_mmlu_pro_20241016_123500.json
â””â”€â”€ summary_20241016_123600.json             # å¤šä»»åŠ¡æ±‡æ€»
```

ç»“æœæ ¼å¼ï¼š
```json
{
  "timestamp": "20241016_123456",
  "dataset": {"name": "lambada", "size": 5153},
  "evaluator": "perplexity",
  "metrics": {
    "perplexity": 15.23,
    "accuracy": 0.45
  }
}
```

---

## ğŸ’¡ å®ç”¨æŠ€å·§

### 1. å¿«é€Ÿæµ‹è¯•ï¼ˆé™åˆ¶æ ·æœ¬ï¼‰
```bash
--limit 100  # åªæµ‹è¯•100ä¸ªæ ·æœ¬
```

### 2. è‡ªå®šä¹‰è¾“å‡ºç›®å½•
```bash
--output-dir ./my_results
```

### 3. ä½¿ç”¨ç¯å¢ƒå˜é‡
```bash
export MODEL_PATH="/path/to/model"
python run_pipeline.py --dataset lambada --dataset-path /path/to/data.jsonl --evaluator perplexity
```

### 4. æ‰¹é‡è¿è¡Œå¤šä¸ªä»»åŠ¡
```python
from pipeline import PipelineBuilder

(PipelineBuilder("/path/to/model")
    .add_task("lambada", "/path/to/lambada.jsonl", "perplexity")
    .add_task("mmlu_pro", "/path/to/mmlu.jsonl", "mmlu_pro")
    .add_task("gsm8k", "/path/to/gsm8k.jsonl", "generation")
    .run())
```

---

## ğŸ“š è¯¦ç»†æ–‡æ¡£

- **ä½¿ç”¨æŒ‡å—**: `USAGE.md`
- **æ·»åŠ æ•°æ®é›†**: `HOW_TO_ADD_NEW_DATASET.md`
- **MMLU Pro ç¤ºä¾‹**: `register_mmlu_pro.py`
- **å®Œæ•´æ–‡æ¡£**: `README.md`

