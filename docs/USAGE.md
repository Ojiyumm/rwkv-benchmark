# è¯„ä¼°ç®¡çº¿ä½¿ç”¨æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æœ€ç®€å•çš„æ–¹å¼ - è¿è¡Œç¤ºä¾‹

```bash
# æŸ¥çœ‹å¯ç”¨çš„æ•°æ®é›†å’Œè¯„ä¼°å™¨
python run_pipeline.py --list

# è¿è¡Œç¤ºä¾‹1: LAMBADAè¯„ä¼°
python run_pipeline.py --example 1

# è¿è¡Œç¤ºä¾‹2: å¤šä»»åŠ¡è¯„ä¼°
python run_pipeline.py --example 2
```

### 2. å‘½ä»¤è¡Œæ–¹å¼

```bash
# å®Œæ•´çš„LAMBADAè¯„ä¼°
python run_pipeline.py \
    --dataset lambada \
    --dataset-path /home/rwkv/Peter/Albatross/eval/lambada_test.jsonl \
    --evaluator perplexity \
    --batch-size 256

# å¿«é€Ÿæµ‹è¯•ï¼ˆåªè·‘100ä¸ªæ ·æœ¬ï¼‰
python run_pipeline.py \
    --dataset lambada \
    --dataset-path /home/rwkv/Peter/Albatross/eval/lambada_test.jsonl \
    --evaluator perplexity \
    --batch-size 256 \
    --limit 100

# æŒ‡å®šæ¨¡å‹è·¯å¾„
python run_pipeline.py \
    --model-path /path/to/your/model \
    --dataset lambada \
    --dataset-path /path/to/lambada.jsonl \
    --evaluator perplexity
```

### 3. Python ä»£ç æ–¹å¼

#### æ–¹å¼A: æœ€ç®€å• - quick_eval

```python
from pipeline import quick_eval

results = quick_eval(
    model_path="/home/rwkv/models/rwkv7/rwkv7-g1-0.4b-20250324-ctx4096",
    dataset_name="lambada",
    dataset_path="/home/rwkv/Peter/Albatross/eval/lambada_test.jsonl",
    evaluator_name="perplexity",
    batch_size=256
)

print(f"Perplexity: {results['metrics']['perplexity']:.2f}")
print(f"Accuracy: {results['metrics']['accuracy']*100:.1f}%")
```

#### æ–¹å¼B: é“¾å¼è°ƒç”¨ - PipelineBuilder

```python
from pipeline import PipelineBuilder

results = (PipelineBuilder(
    model_path="/home/rwkv/models/rwkv7/rwkv7-g1-0.4b-20250324-ctx4096",
    output_dir="./eval_results"
)
.add_task(
    dataset_name="lambada",
    dataset_path="/home/rwkv/Peter/Albatross/eval/lambada_test.jsonl",
    evaluator_name="perplexity",
    batch_size=256
)
.add_task(
    dataset_name="qa",
    dataset_path="/path/to/qa.jsonl",
    evaluator_name="generation",
    max_length=100
)
.run())

print(f"Completed {len(results)} tasks")
```

#### æ–¹å¼C: å®Œå…¨æ§åˆ¶ - EvaluationPipeline

```python
from pipeline import EvaluationPipeline

pipeline = EvaluationPipeline(
    model_path="/home/rwkv/models/rwkv7/rwkv7-g1-0.4b-20250324-ctx4096",
    output_dir="./eval_results",
    seed=42
)

# ä»»åŠ¡1
results1 = pipeline.run(
    dataset_name="lambada",
    dataset_path="/home/rwkv/Peter/Albatross/eval/lambada_test.jsonl",
    evaluator_name="perplexity",
    batch_size=256,
    limit=100  # å¿«é€Ÿæµ‹è¯•
)

# ä»»åŠ¡2
results2 = pipeline.run(
    dataset_name="qa",
    dataset_path="/path/to/qa.jsonl",
    evaluator_name="exact_match",
    batch_size=128
)
```

---

## ğŸ“¦ æ¨¡å—è¯´æ˜

### 1. Dataset Registryï¼ˆæ•°æ®é›†æ³¨å†Œå™¨ï¼‰

**ä½œç”¨**: ç®¡ç†æ•°æ®é›†åŠ è½½å’Œ prompt template

**å†…ç½®æ•°æ®é›†**:
- `lambada` - LAMBADAè¯­è¨€å»ºæ¨¡
- `qa` - é€šç”¨é—®ç­”
- `completion` - æ–‡æœ¬è¡¥å…¨
- `cot` - Chain-of-Thoughtæ¨ç†
- `math` - æ•°å­¦é—®é¢˜

**æ³¨å†Œè‡ªå®šä¹‰æ•°æ®é›†**:

```python
from dataset_registry import DatasetRegistry
import json

# å®šä¹‰åŠ è½½å‡½æ•°
def load_my_data(path: str):
    with open(path, 'r') as f:
        return json.load(f)

# å®šä¹‰ prompt template
def my_prompt_template(item: dict) -> str:
    return f"Question: {item['question']}\nAnswer:"

# æ³¨å†Œ
DatasetRegistry.register(
    name='my_dataset',
    loader=load_my_data,
    prompt_template=my_prompt_template,
    description='My custom dataset',
    default_batch_size=128,
    default_max_length=100
)

# ä½¿ç”¨
results = quick_eval(
    model_path="/path/to/model",
    dataset_name="my_dataset",  # ä½¿ç”¨ä½ æ³¨å†Œçš„åå­—
    dataset_path="/path/to/data.json",
    evaluator_name="generation"
)
```

### 2. Evaluator Registryï¼ˆè¯„ä¼°å™¨æ³¨å†Œå™¨ï¼‰

**ä½œç”¨**: ç®¡ç†ä¸åŒçš„è¯„ä¼°æ–¹æ³•å’ŒæŒ‡æ ‡è®¡ç®—

**å†…ç½®è¯„ä¼°å™¨**:
- `perplexity` - å›°æƒ‘åº¦å’Œå‡†ç¡®ç‡ï¼ˆé€‚ç”¨äºè¯­è¨€å»ºæ¨¡ï¼‰
- `exact_match` - ç²¾ç¡®åŒ¹é…ï¼ˆé€‚ç”¨äºåˆ†ç±»ã€ç®€å•QAï¼‰
- `generation` - æ–‡æœ¬ç”Ÿæˆç»Ÿè®¡ï¼ˆtokensã€throughputç­‰ï¼‰

**æ³¨å†Œè‡ªå®šä¹‰è¯„ä¼°å™¨**:

```python
from evaluator_registry import EvaluatorRegistry

def my_evaluator(engine, data, batch_size=128, **kwargs):
    # ä½ çš„è¯„ä¼°é€»è¾‘
    correct = 0
    total = len(data)
    
    for i in range(0, len(data), batch_size):
        batch = data[i:i+batch_size]
        prompts = [item['prompt'] for item in batch]
        
        # ç”Ÿæˆ
        tokens, _ = engine.generate_batch(prompts, max_length=100)
        predictions = engine.decode_tokens(tokens)
        
        # è¯„ä¼°é€»è¾‘
        for pred, item in zip(predictions, batch):
            if pred.strip() == item['reference'].strip():
                correct += 1
    
    return {
        'accuracy': correct / total,
        'correct': correct,
        'total': total
    }

# æ³¨å†Œ
EvaluatorRegistry.register(
    name='my_evaluator',
    evaluator=my_evaluator,
    description='My custom evaluator',
    metrics=['accuracy', 'correct', 'total']
)

# ä½¿ç”¨
results = quick_eval(
    model_path="/path/to/model",
    dataset_name="lambada",
    dataset_path="/path/to/lambada.jsonl",
    evaluator_name="my_evaluator"  # ä½¿ç”¨ä½ æ³¨å†Œçš„åå­—
)
```

---

## ğŸ¯ å®é™…ä½¿ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1: LAMBADA å®Œæ•´è¯„ä¼°

```bash
python run_pipeline.py \
    --dataset lambada \
    --dataset-path /home/rwkv/Peter/Albatross/eval/lambada_test.jsonl \
    --evaluator perplexity \
    --batch-size 256
```

è¾“å‡ºç»“æœä¼šä¿å­˜åœ¨ `./eval_results/lambada_perplexity_TIMESTAMP.json`

### æ¡ˆä¾‹2: å¤šæ•°æ®é›†æ‰¹é‡è¯„ä¼°

åˆ›å»ºä¸€ä¸ªè„šæœ¬ `my_eval.py`:

```python
from pipeline import PipelineBuilder

model_path = "/home/rwkv/models/rwkv7/rwkv7-g1-0.4b-20250324-ctx4096"

results = (PipelineBuilder(model_path, output_dir="./my_results")
    .add_task("lambada", "/path/to/lambada.jsonl", "perplexity", batch_size=256)
    .add_task("qa", "/path/to/qa.jsonl", "exact_match", batch_size=128)
    .add_task("math", "/path/to/math.json", "generation", batch_size=64, max_length=256)
    .run())

# æ±‡æ€»ç»“æœä¼šä¿å­˜åœ¨ ./my_results/summary_TIMESTAMP.json
```

### æ¡ˆä¾‹3: æ·»åŠ æ–°çš„æ•°å­¦è¯„ä¼°æ•°æ®é›†

```python
from dataset_registry import DatasetRegistry
from pipeline import quick_eval
import json

# 1. æ³¨å†Œæ•°æ®é›†
def load_aime(path: str):
    with open(path, 'r') as f:
        return json.load(f)

def aime_prompt(item: dict) -> str:
    return f"Problem: {item['problem']}\n\nSolution:\n"

DatasetRegistry.register(
    name='aime',
    loader=load_aime,
    prompt_template=aime_prompt,
    description='AIME math problems',
    default_batch_size=64,
    default_max_length=512
)

# 2. è¿è¡Œè¯„ä¼°
results = quick_eval(
    model_path="/home/rwkv/models/rwkv7/rwkv7-g1-0.4b-20250324-ctx4096",
    dataset_name="aime",
    dataset_path="/home/rwkv/Peter/makedata/data/aime_problems.json",
    evaluator_name="generation",
    limit=50  # å…ˆæµ‹è¯•50ä¸ª
)
```

---

## ğŸ“ è¾“å‡ºæ–‡ä»¶è¯´æ˜

### å•ä»»åŠ¡ç»“æœæ–‡ä»¶
`./eval_results/DATASET_EVALUATOR_TIMESTAMP.json`

```json
{
  "timestamp": "20241016_123456",
  "dataset": {
    "name": "lambada",
    "path": "/path/to/lambada.jsonl",
    "size": 5153,
    "limit": null
  },
  "evaluator": "perplexity",
  "config": {
    "batch_size": 256,
    "max_length": 1
  },
  "metrics": {
    "perplexity": 15.23,
    "accuracy": 0.45,
    "total": 5153
  }
}
```

### å¤šä»»åŠ¡æ±‡æ€»æ–‡ä»¶
`./eval_results/summary_TIMESTAMP.json`

åŒ…å«æ‰€æœ‰ä»»åŠ¡çš„ç»“æœåˆ—è¡¨

---

## ğŸ’¡ é«˜çº§æŠ€å·§

### 1. å¿«é€Ÿè°ƒè¯•ï¼ˆé™åˆ¶æ ·æœ¬æ•°ï¼‰

```bash
python run_pipeline.py \
    --dataset lambada \
    --dataset-path /path/to/lambada.jsonl \
    --evaluator perplexity \
    --limit 100  # åªè·‘100ä¸ªæ ·æœ¬
```

### 2. ä¿å­˜ç”Ÿæˆçš„æ–‡æœ¬

```python
results = quick_eval(
    model_path="/path/to/model",
    dataset_name="qa",
    dataset_path="/path/to/qa.jsonl",
    evaluator_name="generation",
    save_outputs=True,  # ä¿å­˜ç”Ÿæˆçš„æ–‡æœ¬
    output_file="qa_outputs.jsonl"
)
```

### 3. ä½¿ç”¨ç¯å¢ƒå˜é‡

```bash
export MODEL_PATH="/home/rwkv/models/rwkv7/rwkv7-g1-0.4b-20250324-ctx4096"

python run_pipeline.py \
    --dataset lambada \
    --dataset-path /path/to/lambada.jsonl \
    --evaluator perplexity
# ä¼šè‡ªåŠ¨ä½¿ç”¨ MODEL_PATH ç¯å¢ƒå˜é‡
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜1: æ•°æ®é›†æœªæ³¨å†Œ
```
ValueError: Dataset 'xxx' not registered
```
**è§£å†³**: è¿è¡Œ `python run_pipeline.py --list` æŸ¥çœ‹å¯ç”¨æ•°æ®é›†

### é—®é¢˜2: è·¯å¾„ä¸å­˜åœ¨
```
FileNotFoundError: [Errno 2] No such file or directory
```
**è§£å†³**: æ£€æŸ¥ `--dataset-path` å’Œ `--model-path` æ˜¯å¦æ­£ç¡®

### é—®é¢˜3: å†…å­˜ä¸è¶³
**è§£å†³**: å‡å° `--batch-size` æˆ–ä½¿ç”¨ `--limit` é™åˆ¶æ ·æœ¬æ•°

