# å¦‚ä½•æ·»åŠ æ–°çš„æ•°æ®é›†å’Œè¯„ä¼°å™¨

## ğŸ“ å¿«é€Ÿæ­¥éª¤

æ·»åŠ æ–°çš„æµ‹è¯„ä»»åŠ¡åªéœ€è¦ **4 æ­¥**ï¼š

1. **å®šä¹‰æ•°æ®åŠ è½½å™¨** - å¦‚ä½•è¯»å–æ•°æ®æ–‡ä»¶
2. **å®šä¹‰ Prompt Template** - å¦‚ä½•æ ¼å¼åŒ–è¾“å…¥
3. **å®šä¹‰è¯„ä¼°å™¨**ï¼ˆå¯é€‰ï¼‰ - å¦‚ä½•è®¡ç®—æŒ‡æ ‡
4. **æ³¨å†Œ** - æ³¨å†Œåˆ°ç³»ç»Ÿä¸­

## ğŸ¯ å®Œæ•´ç¤ºä¾‹ï¼šæ·»åŠ  MMLU Pro

### æ­¥éª¤ 1: åˆ›å»ºæ³¨å†Œæ–‡ä»¶

åˆ›å»º `register_mmlu_pro.py`ï¼š

```python
from dataset_registry import DatasetRegistry
from evaluator_registry import EvaluatorRegistry
import json

# ========== 1. æ•°æ®åŠ è½½å™¨ ==========
def load_mmlu_pro(path: str):
    """åŠ è½½ MMLU Pro æ•°æ®"""
    with open(path, 'r', encoding='utf-8') as f:
        if path.endswith('.jsonl'):
            return [json.loads(line) for line in f]
        else:
            return json.load(f)

# ========== 2. Prompt Template ==========
def mmlu_pro_prompt(item: dict) -> str:
    """æ ¼å¼åŒ–ä¸ºå¤šé€‰é¢˜"""
    question = item['question']
    options = item['options']
    
    # æ„å»ºé€‰é¡¹ A, B, C, D...
    option_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']
    option_text = '\n'.join([
        f"{option_labels[i]}. {opt}" 
        for i, opt in enumerate(options)
    ])
    
    return f"Question: {question}\n\n{option_text}\n\nAnswer:"

# ========== 3. è¯„ä¼°å™¨ï¼ˆå¯é€‰ï¼‰ ==========
def mmlu_pro_evaluator(engine, data, batch_size=64, **kwargs):
    """å¤šé€‰é¢˜è¯„ä¼°"""
    correct = 0
    total = len(data)
    
    option_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']
    
    for i in range(0, len(data), batch_size):
        batch = data[i:i+batch_size]
        prompts = [item['prompt'] for item in batch]
        
        # ç”Ÿæˆç­”æ¡ˆ
        tokens, _ = engine.generate_batch(prompts, max_length=10)
        predictions = engine.decode_tokens(tokens)
        
        # è¯„ä¼°
        for pred, item in zip(predictions, batch):
            # æå–é¢„æµ‹çš„å­—æ¯
            pred_answer = None
            for char in pred.strip().upper():
                if char in option_labels:
                    pred_answer = char
                    break
            
            # è·å–æ­£ç¡®ç­”æ¡ˆ
            correct_answer = item['reference']
            if pred_answer == correct_answer:
                correct += 1
    
    return {
        'accuracy': correct / total,
        'correct': correct,
        'total': total
    }

# ========== 4. æ³¨å†Œ ==========
def register_mmlu_pro():
    # æ³¨å†Œæ•°æ®é›†
    DatasetRegistry.register(
        name='mmlu_pro',
        loader=load_mmlu_pro,
        prompt_template=mmlu_pro_prompt,
        description='MMLU Pro multi-choice questions',
        default_batch_size=64,
        default_max_length=10
    )
    
    # æ³¨å†Œè¯„ä¼°å™¨
    EvaluatorRegistry.register(
        name='mmlu_pro',
        evaluator=mmlu_pro_evaluator,
        description='Multi-choice accuracy',
        metrics=['accuracy', 'correct', 'total']
    )
    
    print("âœ“ MMLU Pro registered!")

# å¯¼å…¥æ—¶è‡ªåŠ¨æ³¨å†Œ
register_mmlu_pro()
```

### æ­¥éª¤ 2: ä½¿ç”¨

**æ–¹æ³• 1: å‘½ä»¤è¡Œ**

```bash
# ç›´æ¥ä½¿ç”¨ï¼ˆä¼šè‡ªåŠ¨æ³¨å†Œï¼‰
python run_pipeline.py \
    --dataset mmlu_pro \
    --dataset-path /path/to/mmlu_pro.jsonl \
    --evaluator mmlu_pro \
    --batch-size 64
```

**æ–¹æ³• 2: Python ä»£ç **

```python
from register_mmlu_pro import register_mmlu_pro  # å¯¼å…¥æ—¶è‡ªåŠ¨æ³¨å†Œ
from pipeline import quick_eval

results = quick_eval(
    model_path="/path/to/model",
    dataset_name="mmlu_pro",
    dataset_path="/path/to/mmlu_pro.jsonl",
    evaluator_name="mmlu_pro"
)
```

**æ–¹æ³• 3: ä¸“ç”¨è„šæœ¬**

```bash
# ä½¿ç”¨ä¸“é—¨ä¸º MMLU Pro åˆ›å»ºçš„è„šæœ¬
python run_mmlu_pro.py \
    --dataset-path /path/to/mmlu_pro.jsonl \
    --batch-size 64 \
    --limit 100  # å¿«é€Ÿæµ‹è¯•
```

---

## ğŸ”§ è¯¦ç»†è¯´æ˜

### 1. æ•°æ®åŠ è½½å™¨å‡½æ•°

æ•°æ®åŠ è½½å™¨è´Ÿè´£è¯»å–æ–‡ä»¶å¹¶è¿”å›æ•°æ®åˆ—è¡¨ï¼š

```python
def load_your_dataset(path: str) -> List[Dict]:
    """
    è¾“å…¥: æ–‡ä»¶è·¯å¾„
    è¾“å‡º: æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸
    """
    # è¯»å–æ–‡ä»¶
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)  # æˆ– [json.loads(line) for line in f]
    
    return data
```

**æ”¯æŒçš„æ•°æ®æ ¼å¼ï¼š**
- JSON: `json.load(f)`
- JSONL: `[json.loads(line) for line in f]`
- CSV: ä½¿ç”¨ `pandas.read_csv()`
- å…¶ä»–æ ¼å¼: è‡ªå®šä¹‰è§£æé€»è¾‘

### 2. Prompt Template å‡½æ•°

Prompt Template è´Ÿè´£å°†æ•°æ®æ ¼å¼åŒ–ä¸ºæ¨¡å‹è¾“å…¥ï¼š

```python
def your_prompt_template(item: Dict) -> str:
    """
    è¾“å…¥: æ•°æ®é¡¹ï¼ˆå­—å…¸ï¼‰
    è¾“å‡º: æ ¼å¼åŒ–çš„ prompt å­—ç¬¦ä¸²
    """
    # ä» item ä¸­æå–å­—æ®µ
    question = item['question']
    context = item.get('context', '')  # å¯é€‰å­—æ®µ
    
    # ç»„åˆæˆ prompt
    if context:
        return f"Context: {context}\n\nQuestion: {question}\nAnswer:"
    else:
        return f"Question: {question}\nAnswer:"
```

**å¸¸è§ Prompt æ¨¡å¼ï¼š**

```python
# 1. ç®€å•é—®ç­”
def qa_prompt(item):
    return f"Q: {item['question']}\nA:"

# 2. æŒ‡ä»¤è·Ÿéš
def instruction_prompt(item):
    return f"{item['instruction']}\n\nInput: {item['input']}\nOutput:"

# 3. å¤šé€‰é¢˜
def multiple_choice_prompt(item):
    options = '\n'.join([f"{i}. {opt}" for i, opt in enumerate(item['options'])])
    return f"{item['question']}\n\n{options}\n\nAnswer:"

# 4. Chain-of-Thought
def cot_prompt(item):
    return f"{item['question']}\n\nLet's think step by step:\n"

# 5. Few-shot
def fewshot_prompt(item):
    examples = "Q: 2+2\nA: 4\n\nQ: 3+3\nA: 6\n\n"
    return f"{examples}Q: {item['question']}\nA:"
```

### 3. è¯„ä¼°å™¨å‡½æ•°ï¼ˆå¯é€‰ï¼‰

å¦‚æœå†…ç½®è¯„ä¼°å™¨ä¸æ»¡è¶³éœ€æ±‚ï¼Œå¯ä»¥è‡ªå®šä¹‰ï¼š

```python
def your_evaluator(engine, data, batch_size=128, **kwargs):
    """
    è¾“å…¥: 
      - engine: æ¨ç†å¼•æ“
      - data: æ•°æ®åˆ—è¡¨ï¼ˆå·²ç»åº”ç”¨äº† prompt templateï¼‰
      - batch_size: æ‰¹é‡å¤§å°
      - **kwargs: å…¶ä»–å‚æ•°
    
    è¾“å‡º: 
      - Dict[str, float]: è¯„ä¼°æŒ‡æ ‡
    """
    correct = 0
    total = len(data)
    
    # æ‰¹é‡å¤„ç†
    for i in range(0, len(data), batch_size):
        batch = data[i:i+batch_size]
        prompts = [item['prompt'] for item in batch]
        
        # ç”Ÿæˆ
        tokens, _ = engine.generate_batch(prompts, max_length=100)
        predictions = engine.decode_tokens(tokens)
        
        # è¯„ä¼°é€»è¾‘
        for pred, item in zip(predictions, batch):
            if your_matching_logic(pred, item['reference']):
                correct += 1
    
    return {
        'accuracy': correct / total,
        'correct': correct,
        'total': total
    }
```

**å†…ç½®è¯„ä¼°å™¨ï¼š**
- `perplexity` - å›°æƒ‘åº¦è¯„ä¼°ï¼ˆè¯­è¨€å»ºæ¨¡ï¼‰
- `exact_match` - ç²¾ç¡®åŒ¹é…ï¼ˆåˆ†ç±»ã€ç®€å•QAï¼‰
- `generation` - ç”Ÿæˆç»Ÿè®¡ï¼ˆthroughputã€tokenæ•°ç­‰ï¼‰

### 4. æ³¨å†Œ

```python
# æ³¨å†Œæ•°æ®é›†
DatasetRegistry.register(
    name='your_dataset',              # æ•°æ®é›†åç§°
    loader=load_your_dataset,         # åŠ è½½å‡½æ•°
    prompt_template=your_prompt,      # prompt æ¨¡æ¿
    description='Dataset description', # æè¿°
    default_batch_size=128,           # é»˜è®¤æ‰¹é‡å¤§å°
    default_max_length=100            # é»˜è®¤æœ€å¤§ç”Ÿæˆé•¿åº¦
)

# æ³¨å†Œè¯„ä¼°å™¨ï¼ˆå¯é€‰ï¼‰
EvaluatorRegistry.register(
    name='your_evaluator',
    evaluator=your_evaluator,
    description='Evaluator description',
    metrics=['accuracy', 'f1', 'etc']
)
```

---

## ğŸ“š æ›´å¤šç¤ºä¾‹

### ç¤ºä¾‹ 1: GSM8K (æ•°å­¦é—®é¢˜)

```python
def load_gsm8k(path: str):
    with open(path, 'r') as f:
        return [json.loads(line) for line in f]

def gsm8k_prompt(item: dict) -> str:
    return f"Question: {item['question']}\n\nAnswer: Let's solve this step by step.\n"

DatasetRegistry.register(
    name='gsm8k',
    loader=load_gsm8k,
    prompt_template=gsm8k_prompt,
    description='GSM8K grade school math',
    default_batch_size=64,
    default_max_length=256
)
```

ä½¿ç”¨ï¼š
```bash
python run_pipeline.py \
    --dataset gsm8k \
    --dataset-path /path/to/gsm8k.jsonl \
    --evaluator generation \
    --batch-size 64 \
    --max-length 256
```

### ç¤ºä¾‹ 2: HumanEval (ä»£ç ç”Ÿæˆ)

```python
def load_humaneval(path: str):
    with open(path, 'r') as f:
        return [json.loads(line) for line in f]

def humaneval_prompt(item: dict) -> str:
    return f"{item['prompt']}\n"  # HumanEval å·²ç»åŒ…å«å®Œæ•´ prompt

DatasetRegistry.register(
    name='humaneval',
    loader=load_humaneval,
    prompt_template=humaneval_prompt,
    description='HumanEval code generation',
    default_batch_size=32,
    default_max_length=512
)
```

### ç¤ºä¾‹ 3: è‡ªå®šä¹‰ JSON æ•°æ®

å‡è®¾ä½ çš„æ•°æ®æ ¼å¼æ˜¯ï¼š
```json
{
  "examples": [
    {
      "input": "Translate to French: Hello",
      "output": "Bonjour",
      "metadata": {"lang": "fr"}
    }
  ]
}
```

æ³¨å†Œï¼š
```python
def load_custom(path: str):
    with open(path, 'r') as f:
        data = json.load(f)
        return data['examples']  # æå– examples æ•°ç»„

def custom_prompt(item: dict) -> str:
    return item['input']  # ç›´æ¥ä½¿ç”¨ input å­—æ®µ

DatasetRegistry.register(
    name='my_translation',
    loader=load_custom,
    prompt_template=custom_prompt,
    description='My translation dataset',
    default_batch_size=128,
    default_max_length=50
)
```

---

## âœ… éªŒè¯

æ³¨å†Œå®Œæˆåï¼ŒéªŒè¯æ˜¯å¦æˆåŠŸï¼š

```python
from dataset_registry import DatasetRegistry
from evaluator_registry import EvaluatorRegistry

# åˆ—å‡ºæ‰€æœ‰æ•°æ®é›†
print("Datasets:", DatasetRegistry.list_datasets())

# åˆ—å‡ºæ‰€æœ‰è¯„ä¼°å™¨
print("Evaluators:", EvaluatorRegistry.list_evaluators())

# æµ‹è¯•åŠ è½½
data = DatasetRegistry.load_dataset(
    name='mmlu_pro',
    path='/path/to/mmlu_pro.jsonl',
    limit=5  # åªåŠ è½½5ä¸ªæ ·æœ¬æµ‹è¯•
)

print(f"Loaded {len(data)} samples")
print(f"First prompt: {data[0]['prompt'][:100]}...")
```

---

## ğŸ¯ æœ€ä½³å®è·µ

1. **æ•°æ®åŠ è½½å™¨**ï¼šå°½é‡ä¿æŒç®€å•ï¼Œåªè´Ÿè´£è¯»å–å’Œè§£æ
2. **Prompt Template**ï¼šæ ¹æ®ä»»åŠ¡ç‰¹ç‚¹è®¾è®¡ï¼Œå¯ä»¥åˆ›å»ºå¤šä¸ªç‰ˆæœ¬ï¼ˆæ ‡å‡†ç‰ˆã€CoTç‰ˆç­‰ï¼‰
3. **è¯„ä¼°å™¨**ï¼šä¼˜å…ˆä½¿ç”¨å†…ç½®è¯„ä¼°å™¨ï¼Œç‰¹æ®Šéœ€æ±‚å†è‡ªå®šä¹‰
4. **å‘½åè§„èŒƒ**ï¼šä½¿ç”¨æ¸…æ™°çš„åç§°ï¼Œå¦‚ `dataset_name`, `dataset_name_cot`
5. **æµ‹è¯•**ï¼šä½¿ç”¨ `--limit` å‚æ•°å…ˆæµ‹è¯•å°é‡æ•°æ®
6. **æ–‡æ¡£**ï¼šåœ¨æ³¨å†Œæ—¶å¡«å†™æ¸…æ™°çš„ `description`

---

## ğŸ’¡ æç¤º

- ä¸€ä¸ªæ•°æ®é›†å¯ä»¥æ³¨å†Œå¤šä¸ªç‰ˆæœ¬ï¼ˆä¸åŒçš„ promptï¼‰
- æ•°æ®é›†å’Œè¯„ä¼°å™¨å¯ä»¥ç»„åˆä½¿ç”¨
- ä½¿ç”¨ `limit` å‚æ•°å¿«é€Ÿè°ƒè¯•
- æŸ¥çœ‹ `register_mmlu_pro.py` è·å–å®Œæ•´ç¤ºä¾‹

