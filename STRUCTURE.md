# é¡¹ç›®ç»“æ„è¯´æ˜

## ğŸ“‚ å®Œæ•´ç›®å½•æ ‘

```
rwkveval/
â”‚
â”œâ”€â”€ local_batch_benchmark/          # æœ¬åœ°æ‰¹é‡è¯„ä¼°æ¨¡å—
â”‚   â”‚
â”‚   â”œâ”€â”€ datasets/                   # æ•°æ®é›†æ¨¡å—ï¼ˆæ³¨å†Œå™¨æ¨¡å¼ï¼‰
â”‚   â”‚   â”œâ”€â”€ __init__.py            # è‡ªåŠ¨å¯¼å…¥æ‰€æœ‰æ•°æ®é›†
â”‚   â”‚   â”œâ”€â”€ dataset_registry.py     # æ ¸å¿ƒæ³¨å†Œå™¨
â”‚   â”‚   â””â”€â”€ register_mmlu_pro.py   # MMLU Pro æ•°æ®é›†
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluators/                 # è¯„ä¼°å™¨æ¨¡å—ï¼ˆæ³¨å†Œå™¨æ¨¡å¼ï¼‰
â”‚   â”‚   â”œâ”€â”€ __init__.py            # è‡ªåŠ¨å¯¼å…¥æ‰€æœ‰è¯„ä¼°å™¨
â”‚   â”‚   â”œâ”€â”€ evaluator_registry.py   # æ ¸å¿ƒæ³¨å†Œå™¨
â”‚   â”‚   â””â”€â”€ register_mmlu_pro.py   # MMLU Pro è¯„ä¼°å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ batch_engine.py            # RWKV æ¨ç†å¼•æ“
â”‚   â”œâ”€â”€ pipeline.py                # è¯„ä¼°ç®¡çº¿æ ¸å¿ƒ
â”‚   â”œâ”€â”€ run_pipeline.py            # å‘½ä»¤è¡Œå…¥å£
â”‚   â”œâ”€â”€ run_mmlu_pro.py           # MMLU Pro ä¸“ç”¨è„šæœ¬
â”‚   â”œâ”€â”€ example_usage.py          # ä½¿ç”¨ç¤ºä¾‹
â”‚   â””â”€â”€ README.md                 # æ¨¡å—æ–‡æ¡£
â”‚
â”œâ”€â”€ api_benchmark/                 # API è¯„ä¼°æ¨¡å—
â”‚   â”œâ”€â”€ api_server.py             # FastAPI æœåŠ¡å™¨
â”‚   â”œâ”€â”€ api.sh                    # å¯åŠ¨è„šæœ¬
â”‚   â”œâ”€â”€ batch_engine.py           # RWKV æ¨ç†å¼•æ“
â”‚   â”œâ”€â”€ run_eval.sh               # lm_eval è¯„ä¼°è„šæœ¬
â”‚   â””â”€â”€ test_lambada_api.py       # API æµ‹è¯•
â”‚
â”œâ”€â”€ reference/                     # RWKV å‚è€ƒå®ç°
â”‚   â”œâ”€â”€ rwkv7.py                  # RWKV-7 æ¨¡å‹
â”‚   â”œâ”€â”€ utils.py                  # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ rwkv_vocab_v20230424.txt  # è¯æ±‡è¡¨
â”‚
â”œâ”€â”€ docs/                          # æ–‡æ¡£ç›®å½•
â”œâ”€â”€ tools/                         # å·¥å…·è„šæœ¬
â”œâ”€â”€ README.md                      # é¡¹ç›®ä¸»æ–‡æ¡£
â””â”€â”€ STRUCTURE.md                   # æœ¬æ–‡ä»¶
```

## ğŸ¯ è®¾è®¡ç†å¿µ

### 1. æ¨¡å—åŒ–åˆ†ç¦»
- **æœ¬åœ°æ‰¹é‡è¯„ä¼°** å’Œ **APIè¯„ä¼°** å®Œå…¨åˆ†ç¦»
- äº’ä¸å¹²æ‰°ï¼Œå„è‡ªç‹¬ç«‹è¿è¡Œ

### 2. æ³¨å†Œå™¨æ¨¡å¼
- **æ•°æ®é›†æ³¨å†Œå™¨** (`datasets/`)ï¼šç®¡ç†æ‰€æœ‰æ•°æ®é›†çš„åŠ è½½å’Œ prompt æ¨¡æ¿
- **è¯„ä¼°å™¨æ³¨å†Œå™¨** (`evaluators/`)ï¼šç®¡ç†æ‰€æœ‰è¯„ä¼°æ–¹æ³•å’ŒæŒ‡æ ‡è®¡ç®—

### 3. è‡ªåŠ¨å¯¼å…¥
- åœ¨ `datasets/__init__.py` å’Œ `evaluators/__init__.py` ä¸­è‡ªåŠ¨å¯¼å…¥æ‰€æœ‰æ³¨å†Œæ–‡ä»¶
- åªéœ€å¯¼å…¥æ¨¡å—ï¼Œæ‰€æœ‰ç»„ä»¶è‡ªåŠ¨å¯ç”¨

## ğŸ”„ å·¥ä½œæµç¨‹

### æ•°æ®é›†æ³¨å†Œæµç¨‹

```
1. åˆ›å»ºæ³¨å†Œæ–‡ä»¶
   datasets/register_xxx.py
   
2. å®šä¹‰åŠ è½½å™¨å’Œæ¨¡æ¿
   def load_xxx(path) -> List[Dict]
   def xxx_prompt(item) -> str
   
3. è°ƒç”¨æ³¨å†Œ
   DatasetRegistry.register(name, loader, template, ...)
   
4. åœ¨ __init__.py å¯¼å…¥
   from . import register_xxx
   
5. è‡ªåŠ¨å¯ç”¨
   DatasetRegistry.list_datasets()  # åŒ…å«æ–°æ•°æ®é›†
```

### è¯„ä¼°å™¨æ³¨å†Œæµç¨‹

```
1. åˆ›å»ºæ³¨å†Œæ–‡ä»¶
   evaluators/register_xxx.py
   
2. å®šä¹‰è¯„ä¼°å‡½æ•°
   def xxx_evaluator(engine, data, **kwargs) -> Dict
   
3. è°ƒç”¨æ³¨å†Œ
   EvaluatorRegistry.register(name, evaluator, ...)
   
4. åœ¨ __init__.py å¯¼å…¥
   from . import register_xxx
   
5. è‡ªåŠ¨å¯ç”¨
   EvaluatorRegistry.list_evaluators()  # åŒ…å«æ–°è¯„ä¼°å™¨
```

## ğŸ“ æ–‡ä»¶èŒè´£

### local_batch_benchmark/

| æ–‡ä»¶ | èŒè´£ |
|------|------|
| `datasets/dataset_registry.py` | æ•°æ®é›†æ³¨å†Œå™¨æ ¸å¿ƒï¼Œæä¾›æ³¨å†Œã€è·å–ã€åŠ è½½åŠŸèƒ½ |
| `datasets/register_*.py` | å…·ä½“æ•°æ®é›†çš„æ³¨å†Œï¼ˆåŠ è½½å™¨ + promptæ¨¡æ¿ï¼‰ |
| `datasets/__init__.py` | è‡ªåŠ¨å¯¼å…¥æ‰€æœ‰æ•°æ®é›†æ³¨å†Œ |
| `evaluators/evaluator_registry.py` | è¯„ä¼°å™¨æ³¨å†Œå™¨æ ¸å¿ƒï¼Œæä¾›æ³¨å†Œã€è·å–ã€è¿è¡ŒåŠŸèƒ½ |
| `evaluators/register_*.py` | å…·ä½“è¯„ä¼°å™¨çš„æ³¨å†Œï¼ˆè¯„ä¼°å‡½æ•°ï¼‰ |
| `evaluators/__init__.py` | è‡ªåŠ¨å¯¼å…¥æ‰€æœ‰è¯„ä¼°å™¨æ³¨å†Œ |
| `batch_engine.py` | RWKVæ¨ç†å¼•æ“ï¼Œæä¾›æ‰¹é‡æ¨ç†èƒ½åŠ› |
| `pipeline.py` | è¯„ä¼°ç®¡çº¿ï¼Œä¸²è”æ•°æ®åŠ è½½ã€æ¨ç†ã€è¯„ä¼° |
| `run_pipeline.py` | å‘½ä»¤è¡Œå…¥å£ï¼Œæä¾›å‹å¥½çš„CLI |
| `run_*.py` | ç‰¹å®šä»»åŠ¡çš„å¿«æ·è„šæœ¬ |
| `example_usage.py` | Python API ä½¿ç”¨ç¤ºä¾‹ |

### api_benchmark/

| æ–‡ä»¶ | èŒè´£ |
|------|------|
| `api_server.py` | FastAPIæœåŠ¡å™¨ï¼Œæä¾›OpenAIå…¼å®¹çš„API |
| `api.sh` | APIå¯åŠ¨è„šæœ¬ |
| `batch_engine.py` | RWKVæ¨ç†å¼•æ“ï¼ˆä¸localç‰ˆæœ¬åŠŸèƒ½ç›¸åŒï¼‰ |
| `run_eval.sh` | lm_evalè¯„ä¼°è„šæœ¬ |
| `test_*.py` | APIæµ‹è¯•è„šæœ¬ |

## ğŸ”— ä¾èµ–å…³ç³»

```
run_pipeline.py
    â”œâ”€â”€ pipeline.py
    â”‚   â”œâ”€â”€ batch_engine.py
    â”‚   â”œâ”€â”€ datasets (module)
    â”‚   â”‚   â””â”€â”€ dataset_registry.py
    â”‚   â””â”€â”€ evaluators (module)
    â”‚       â””â”€â”€ evaluator_registry.py
    â””â”€â”€ datasets/__init__.py (è‡ªåŠ¨æ³¨å†Œ)
    â””â”€â”€ evaluators/__init__.py (è‡ªåŠ¨æ³¨å†Œ)
```

## ğŸŒŸ æ ¸å¿ƒç±»

### DatasetRegistry (datasets/dataset_registry.py)
```python
class DatasetRegistry:
    @classmethod
    def register(cls, name, loader, prompt_template, ...)
    
    @classmethod
    def get(cls, name) -> DatasetConfig
    
    @classmethod
    def load_dataset(cls, name, path) -> List[Dict]
    
    @classmethod
    def list_datasets(cls) -> List[str]
```

### EvaluatorRegistry (evaluators/evaluator_registry.py)
```python
class EvaluatorRegistry:
    @classmethod
    def register(cls, name, evaluator, ...)
    
    @classmethod
    def get(cls, name) -> EvaluatorConfig
    
    @classmethod
    def evaluate(cls, name, engine, data) -> Dict
    
    @classmethod
    def list_evaluators(cls) -> List[str]
```

### EvaluationPipeline (pipeline.py)
```python
class EvaluationPipeline:
    def __init__(self, model_path, ...)
    
    def run(self, dataset_name, dataset_path, evaluator_name, ...) -> Dict
    
    def run_multiple(self, tasks) -> List[Dict]
```

## ğŸ’¡ æ·»åŠ æ–°ç»„ä»¶çš„æ­¥éª¤

### æ·»åŠ æ–°æ•°æ®é›†

1. åœ¨ `local_batch_benchmark/datasets/` åˆ›å»º `register_your_dataset.py`
2. å®šä¹‰ `load_your_dataset()` å’Œ `your_prompt_template()`
3. è°ƒç”¨ `DatasetRegistry.register()`
4. åœ¨ `datasets/__init__.py` æ·»åŠ  `from . import register_your_dataset`
5. è¿è¡Œ `python run_pipeline.py --list` éªŒè¯

### æ·»åŠ æ–°è¯„ä¼°å™¨

1. åœ¨ `local_batch_benchmark/evaluators/` åˆ›å»º `register_your_evaluator.py`
2. å®šä¹‰ `your_evaluator(engine, data, **kwargs)`
3. è°ƒç”¨ `EvaluatorRegistry.register()`
4. åœ¨ `evaluators/__init__.py` æ·»åŠ  `from . import register_your_evaluator`
5. è¿è¡Œ `python run_pipeline.py --list` éªŒè¯

## ğŸ¨ è®¾è®¡æ¨¡å¼

- **æ³¨å†Œå™¨æ¨¡å¼**: é›†ä¸­ç®¡ç†æ•°æ®é›†å’Œè¯„ä¼°å™¨
- **æ¨¡æ¿æ–¹æ³•æ¨¡å¼**: å®šä¹‰è¯„ä¼°æµç¨‹çš„æ¡†æ¶
- **ç­–ç•¥æ¨¡å¼**: å¯æ’æ‹”çš„æ•°æ®åŠ è½½å™¨å’Œè¯„ä¼°å™¨
- **å·¥å‚æ¨¡å¼**: é€šè¿‡åç§°åˆ›å»ºæ•°æ®é›†å’Œè¯„ä¼°å™¨å®ä¾‹

## ğŸ“¦ åŒ…ç»„ç»‡

```python
# ç”¨æˆ·ä»£ç 
from datasets import DatasetRegistry      # è‡ªåŠ¨æ³¨å†Œæ‰€æœ‰æ•°æ®é›†
from evaluators import EvaluatorRegistry  # è‡ªåŠ¨æ³¨å†Œæ‰€æœ‰è¯„ä¼°å™¨
from pipeline import quick_eval           # å¿«é€Ÿè¯„ä¼°æ¥å£

# è¿è¡Œè¯„ä¼°
results = quick_eval(
    model_path="/path/to/model",
    dataset_name="lambada",        # å·²æ³¨å†Œçš„æ•°æ®é›†åç§°
    dataset_path="/path/to/data",
    evaluator_name="perplexity"    # å·²æ³¨å†Œçš„è¯„ä¼°å™¨åç§°
)
```

## ğŸ” æŸ¥æ‰¾æŒ‡å—

éœ€è¦... | æŸ¥æ‰¾ä½ç½®
--------|----------
æ·»åŠ æ–°æ•°æ®é›† | `local_batch_benchmark/datasets/register_*.py`
æ·»åŠ æ–°è¯„ä¼°å™¨ | `local_batch_benchmark/evaluators/register_*.py`
ä¿®æ”¹æ¨ç†é€»è¾‘ | `batch_engine.py`
ä¿®æ”¹è¯„ä¼°æµç¨‹ | `pipeline.py`
ä¿®æ”¹CLI | `run_pipeline.py`
APIæœåŠ¡å™¨ | `api_benchmark/api_server.py`
lm_evalé›†æˆ | `api_benchmark/run_eval.sh`

## ğŸ¯ æœ€ä½³å®è·µ

1. **æ•°æ®é›†å‘½å**: ä½¿ç”¨å°å†™ä¸‹åˆ’çº¿ï¼Œå¦‚ `lambada`, `mmlu_pro`
2. **è¯„ä¼°å™¨å‘½å**: æè¿°æ€§åç§°ï¼Œå¦‚ `perplexity`, `exact_match`
3. **æ–‡ä»¶å‘½å**: `register_<name>.py` è¡¨ç¤ºæ³¨å†Œæ–‡ä»¶
4. **å¯¼å…¥é¡ºåº**: å…ˆå¯¼å…¥åŸºç±»ï¼Œå†å¯¼å…¥æ³¨å†Œ
5. **é”™è¯¯å¤„ç†**: åœ¨ `__init__.py` ä¸­ä½¿ç”¨ try-except æ•è·å¯¼å…¥é”™è¯¯

## ğŸ“š ç›¸å…³æ–‡æ¡£

- `README.md` - é¡¹ç›®æ€»è§ˆ
- `local_batch_benchmark/README.md` - æœ¬åœ°è¯„ä¼°è¯¦ç»†æ–‡æ¡£
- `example_usage.py` - ä»£ç ç¤ºä¾‹

