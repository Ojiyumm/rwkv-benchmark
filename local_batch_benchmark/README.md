# RWKV æœ¬åœ°æ‰¹é‡è¯„ä¼°ç³»ç»Ÿ

ä¸€ä¸ªæ¨¡å—åŒ–çš„æœ¬åœ°è¯„ä¼°ç®¡çº¿ï¼Œç”¨äºé«˜æ•ˆè¿è¡Œ RWKV æ¨¡å‹çš„æ‰¹é‡æ¨ç†å’Œè¯„ä¼°ä»»åŠ¡ã€‚

## ğŸ“‘ ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [é¡¹ç›®ç»“æ„](#-é¡¹ç›®ç»“æ„)
- [è¯„ä¼°æµç¨‹](#-è¯„ä¼°æµç¨‹)
- [ä½¿ç”¨æ–¹æ³•](#-ä½¿ç”¨æ–¹æ³•)
- [æ·»åŠ æ–°æ•°æ®é›†](#-æ·»åŠ æ–°æ•°æ®é›†)
- [æ·»åŠ æ–°è¯„ä¼°å™¨](#-æ·»åŠ æ–°è¯„ä¼°å™¨)
- [å†…ç½®ç»„ä»¶](#-å†…ç½®ç»„ä»¶)
- [Python API](#-python-api)
- [å¸¸è§é—®é¢˜](#-å¸¸è§é—®é¢˜)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åˆ—å‡ºæ‰€æœ‰å¯ç”¨ç»„ä»¶

```bash
cd /home/rwkv/Peter/rwkveval/local_batch_benchmark
python3 run_pipeline.py --list
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
Registered Datasets:
  - lambada (LAMBADA language modeling)
  - mmlu_pro (MMLU Pro - Multi-task Language Understanding)
  - mmlu_pro_cot (MMLU Pro with Chain-of-Thought)
  ...

Registered Evaluators:
  - perplexity (Perplexity and accuracy evaluation)
  - mmlu_pro (MMLU Pro evaluator with subject-wise statistics)
  ...
```

### 2. è¿è¡Œ MMLU-Pro è¯„ä¼°ï¼ˆæ¨èï¼‰

```bash
# å¿«é€Ÿæµ‹è¯•ï¼ˆ100ä¸ªæ ·æœ¬ï¼‰
python3 run_mmlu_pro.py \
    --dataset-path /home/rwkv/Peter/rwkveval/benchmarkdata/mmlupro \
    --batch-size 64 \
    --limit 100

# å®Œæ•´è¯„ä¼°ï¼ˆ12,032ä¸ªæ ·æœ¬ï¼‰
python3 run_mmlu_pro.py \
    --dataset-path /home/rwkv/Peter/rwkveval/benchmarkdata/mmlupro \
    --batch-size 64

# ä¿å­˜æ¨ç†ç»“æœåˆ°æ–‡ä»¶
python3 run_mmlu_pro.py \
    --dataset-path /home/rwkv/Peter/rwkveval/benchmarkdata/mmlupro \
    --batch-size 64 \
    --inferoutput ./results/mmlu_pro_inference.jsonl
```

### 3. è¿è¡Œ LAMBADA è¯„ä¼°

```bash
python3 run_pipeline.py \
    --dataset lambada \
    --dataset-path /path/to/lambada_test.jsonl \
    --evaluator perplexity \
    --batch-size 256 \
    --limit 100
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
local_batch_benchmark/
â”‚
â”œâ”€â”€ tasks/                          # æ•°æ®é›†æ³¨å†Œæ¨¡å—ï¼ˆåŸ datasets/ï¼‰
â”‚   â”œâ”€â”€ __init__.py                 # è‡ªåŠ¨å¯¼å…¥æ‰€æœ‰æ•°æ®é›†
â”‚   â”œâ”€â”€ dataset_registry.py         # æ•°æ®é›†æ³¨å†Œå™¨æ ¸å¿ƒ
â”‚   â””â”€â”€ register_mmlu_pro.py        # MMLU Pro æ•°æ®é›†æ³¨å†Œç¤ºä¾‹
â”‚
â”œâ”€â”€ evaluators/                     # è¯„ä¼°å™¨æ³¨å†Œæ¨¡å—
â”‚   â”œâ”€â”€ __init__.py                 # è‡ªåŠ¨å¯¼å…¥æ‰€æœ‰è¯„ä¼°å™¨
â”‚   â”œâ”€â”€ evaluator_registry.py       # è¯„ä¼°å™¨æ³¨å†Œå™¨æ ¸å¿ƒ
â”‚   â””â”€â”€ register_mmlu_pro.py        # MMLU Pro è¯„ä¼°å™¨æ³¨å†Œç¤ºä¾‹
â”‚
â”œâ”€â”€ batch_engine.py                 # RWKV æ‰¹é‡æ¨ç†å¼•æ“
â”œâ”€â”€ pipeline.py                     # è¯„ä¼°ç®¡çº¿æ ¸å¿ƒ
â”œâ”€â”€ run_pipeline.py                 # é€šç”¨å‘½ä»¤è¡Œå…¥å£
â”œâ”€â”€ run_mmlu_pro.py                 # MMLU Pro ä¸“ç”¨è¿è¡Œè„šæœ¬
â”œâ”€â”€ example_usage.py                # ä½¿ç”¨ç¤ºä¾‹ï¼ˆPython APIï¼‰
â””â”€â”€ README.md                       # æœ¬æ–‡ä»¶
```

### æ ¸å¿ƒæ¨¡å—è¯´æ˜

#### 1. `batch_engine.py` - æ¨ç†å¼•æ“
- **åŠŸèƒ½**ï¼šåŠ è½½ RWKV æ¨¡å‹ï¼Œæ‰§è¡Œæ‰¹é‡æ¨ç†
- **ä¸»è¦ç±»**ï¼š`RWKVInferenceEngine`
- **æ ¸å¿ƒæ–¹æ³•**ï¼š
  - `generate_batch()`: æ‰¹é‡ç”Ÿæˆæ–‡æœ¬
  - `decode_tokens()`: è§£ç  token ä¸ºæ–‡æœ¬
  - `generate_with_logprobs()`: ç”Ÿæˆå¹¶è¿”å› log æ¦‚ç‡

#### 2. `tasks/` - æ•°æ®é›†æ¨¡å—
- **åŠŸèƒ½**ï¼šç®¡ç†æ•°æ®é›†åŠ è½½å’Œ prompt æ¨¡æ¿
- **æ³¨å†Œæ¨¡å¼**ï¼šä½¿ç”¨ `DatasetRegistry` æ³¨å†Œæ•°æ®é›†
- **èŒè´£**ï¼š
  - ä»æ–‡ä»¶åŠ è½½åŸå§‹æ•°æ®
  - åº”ç”¨ prompt æ¨¡æ¿æ ¼å¼åŒ–æ•°æ®
  - æä¾›é»˜è®¤æ‰¹å¤„ç†å‚æ•°

#### 3. `evaluators/` - è¯„ä¼°å™¨æ¨¡å—
- **åŠŸèƒ½**ï¼šæ‰§è¡Œå…·ä½“çš„è¯„ä¼°é€»è¾‘
- **æ³¨å†Œæ¨¡å¼**ï¼šä½¿ç”¨ `EvaluatorRegistry` æ³¨å†Œè¯„ä¼°å™¨
- **èŒè´£**ï¼š
  - æ‰¹é‡æ¨ç†
  - è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€å›°æƒ‘åº¦ç­‰ï¼‰
  - è¿”å›ç»“æ„åŒ–ç»“æœ

#### 4. `pipeline.py` - ç®¡çº¿æ ¸å¿ƒ
- **åŠŸèƒ½**ï¼šä¸²è”æ•°æ®é›†ã€æ¨ç†å¼•æ“ã€è¯„ä¼°å™¨
- **ä¸»è¦ç±»**ï¼š
  - `EvaluationPipeline`: å®Œæ•´è¯„ä¼°ç®¡çº¿
  - `PipelineBuilder`: é“¾å¼æ„å»ºå¤šä»»åŠ¡è¯„ä¼°
- **æ ¸å¿ƒå‡½æ•°**ï¼š
  - `quick_eval()`: å¿«é€Ÿè¿è¡Œå•ä¸ªè¯„ä¼°

---

## ğŸ”„ è¯„ä¼°æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è¯„ä¼°å®Œæ•´æµç¨‹                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. åˆå§‹åŒ–é˜¶æ®µ
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  åŠ è½½ RWKV æ¨¡å‹                      â”‚
   â”‚  â””â”€ RWKVInferenceEngine.__init__()  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
2. æ•°æ®åŠ è½½é˜¶æ®µ
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  DatasetRegistry.load_dataset()     â”‚
   â”‚  â”œâ”€ è°ƒç”¨æ³¨å†Œçš„ loader å‡½æ•°          â”‚
   â”‚  â”œâ”€ åº”ç”¨ prompt_template            â”‚
   â”‚  â””â”€ è¿”å›æ ¼å¼åŒ–çš„æ•°æ®                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
3. æ‰¹é‡æ¨ç†é˜¶æ®µ
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  EvaluatorRegistry.evaluate()       â”‚
   â”‚  â”œâ”€ åˆ†æ‰¹å¤„ç†æ•°æ® (batch_size)       â”‚
   â”‚  â”œâ”€ engine.generate_batch()         â”‚
   â”‚  â”œâ”€ engine.decode_tokens()          â”‚
   â”‚  â””â”€ æ”¶é›†æ‰€æœ‰é¢„æµ‹ç»“æœ                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
4. è¯„ä¼°è®¡ç®—é˜¶æ®µ
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  è¯„ä¼°å™¨è®¡ç®—æŒ‡æ ‡                      â”‚
   â”‚  â”œâ”€ æ¯”è¾ƒé¢„æµ‹ä¸å‚è€ƒç­”æ¡ˆ              â”‚
   â”‚  â”œâ”€ è®¡ç®—å‡†ç¡®ç‡/å›°æƒ‘åº¦ç­‰æŒ‡æ ‡         â”‚
   â”‚  â””â”€ ç”Ÿæˆè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
5. ç»“æœä¿å­˜é˜¶æ®µ
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  ä¿å­˜ç»“æœ                            â”‚
   â”‚  â”œâ”€ è¯„ä¼°æŒ‡æ ‡ â†’ JSON æ–‡ä»¶            â”‚
   â”‚  â”œâ”€ æ¨ç†è¯¦æƒ… â†’ JSONL æ–‡ä»¶ (å¯é€‰)    â”‚
   â”‚  â””â”€ æ‰“å°æ‘˜è¦åˆ°ç»ˆç«¯                  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“– ä½¿ç”¨æ–¹æ³•

### æ–¹å¼ 1: ä½¿ç”¨é€šç”¨å‘½ä»¤è¡Œå·¥å…·

```bash
python3 run_pipeline.py \
    --dataset <æ•°æ®é›†åç§°> \
    --dataset-path <æ•°æ®é›†è·¯å¾„> \
    --evaluator <è¯„ä¼°å™¨åç§°> \
    --model-path <æ¨¡å‹è·¯å¾„> \
    --batch-size <æ‰¹å¤„ç†å¤§å°> \
    --max-length <æœ€å¤§ç”Ÿæˆé•¿åº¦> \
    --limit <æ ·æœ¬æ•°é‡é™åˆ¶> \
    --inferoutput <æ¨ç†ç»“æœä¿å­˜è·¯å¾„>
```

**å‚æ•°è¯´æ˜ï¼š**

| å‚æ•° | å¿…éœ€ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|------|
| `--dataset` | âœ… | æ•°æ®é›†åç§°ï¼ˆå·²æ³¨å†Œï¼‰ | `mmlu_pro` |
| `--dataset-path` | âœ… | æ•°æ®é›†æ–‡ä»¶è·¯å¾„ | `/path/to/data` |
| `--evaluator` | âœ… | è¯„ä¼°å™¨åç§°ï¼ˆå·²æ³¨å†Œï¼‰ | `mmlu_pro` |
| `--model-path` | âŒ | æ¨¡å‹è·¯å¾„ | `/path/to/model.pth` |
| `--batch-size` | âŒ | æ‰¹å¤„ç†å¤§å° | `64` |
| `--max-length` | âŒ | æœ€å¤§ç”Ÿæˆé•¿åº¦ | `100` |
| `--limit` | âŒ | é™åˆ¶æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰ | `100` |
| `--inferoutput` | âŒ | ä¿å­˜æ¨ç†ç»“æœçš„è·¯å¾„ | `./results.jsonl` |
| `--seed` | âŒ | éšæœºç§å­ | `42` |

**ç¤ºä¾‹ï¼š**

```bash
# ç¤ºä¾‹ 1: LAMBADA è¯„ä¼°
python3 run_pipeline.py \
    --dataset lambada \
    --dataset-path /path/to/lambada_test.jsonl \
    --evaluator perplexity \
    --batch-size 256 \
    --limit 500

# ç¤ºä¾‹ 2: MMLU-Pro è¯„ä¼°ï¼ˆä¿å­˜æ¨ç†ç»“æœï¼‰
python3 run_pipeline.py \
    --dataset mmlu_pro \
    --dataset-path /path/to/mmlupro \
    --evaluator mmlu_pro \
    --batch-size 64 \
    --inferoutput ./results/mmlu_pro_predictions.jsonl
```

### æ–¹å¼ 2: ä½¿ç”¨ MMLU-Pro ä¸“ç”¨è„šæœ¬

```bash
python3 run_mmlu_pro.py \
    --dataset-path <æ•°æ®é›†è·¯å¾„> \
    --model-path <æ¨¡å‹è·¯å¾„> \
    --batch-size <æ‰¹å¤„ç†å¤§å°> \
    --max-length <æœ€å¤§ç”Ÿæˆé•¿åº¦> \
    --cot \
    --limit <æ ·æœ¬æ•°é‡é™åˆ¶> \
    --inferoutput <æ¨ç†ç»“æœä¿å­˜è·¯å¾„>
```

**å‚æ•°è¯´æ˜ï¼š**

| å‚æ•° | å¿…éœ€ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| `--dataset-path` | âœ… | MMLU-Pro æ•°æ®é›†è·¯å¾„ | - |
| `--model-path` | âŒ | æ¨¡å‹è·¯å¾„ | `/home/rwkv/models/...` |
| `--batch-size` | âŒ | æ‰¹å¤„ç†å¤§å° | `64` |
| `--max-length` | âŒ | æœ€å¤§ç”Ÿæˆé•¿åº¦ | `10` |
| `--cot` | âŒ | ä½¿ç”¨ Chain-of-Thought | `False` |
| `--limit` | âŒ | é™åˆ¶æ ·æœ¬æ•° | å…¨éƒ¨ |
| `--inferoutput` | âŒ | ä¿å­˜æ¨ç†ç»“æœ | ä¸ä¿å­˜ |

**ç¤ºä¾‹ï¼š**

```bash
# æ ‡å‡† MMLU-Pro è¯„ä¼°
python3 run_mmlu_pro.py \
    --dataset-path /home/rwkv/Peter/rwkveval/benchmarkdata/mmlupro \
    --batch-size 64

# ä½¿ç”¨ Chain-of-Thought
python3 run_mmlu_pro.py \
    --dataset-path /home/rwkv/Peter/rwkveval/benchmarkdata/mmlupro \
    --batch-size 32 \
    --max-length 200 \
    --cot

# å¿«é€Ÿæµ‹è¯• + ä¿å­˜ç»“æœ
python3 run_mmlu_pro.py \
    --dataset-path /home/rwkv/Peter/rwkveval/benchmarkdata/mmlupro \
    --batch-size 64 \
    --limit 100 \
    --inferoutput ./debug_results.jsonl
```

### æ–¹å¼ 3: ä½¿ç”¨ Python API

**ç®€å•è¯„ä¼°ï¼š**

```python
from pipeline import quick_eval

results = quick_eval(
    model_path="/path/to/model",
    dataset_name="mmlu_pro",
    dataset_path="/path/to/mmlupro",
    evaluator_name="mmlu_pro",
    batch_size=64,
    limit=100,
    inferoutput="./results.jsonl"  # å¯é€‰
)

print(f"Accuracy: {results['metrics']['accuracy']*100:.2f}%")
```

**é“¾å¼æ„å»ºå¤šä»»åŠ¡ï¼š**

```python
from pipeline import PipelineBuilder

builder = PipelineBuilder(
    model_path="/path/to/model",
    output_dir="./eval_results"
)

builder.add_task(
    dataset_name="lambada",
    dataset_path="/path/to/lambada.jsonl",
    evaluator_name="perplexity",
    batch_size=256
).add_task(
    dataset_name="mmlu_pro",
    dataset_path="/path/to/mmlupro",
    evaluator_name="mmlu_pro",
    batch_size=64
).run()
```

**å®Œæ•´è‡ªå®šä¹‰ï¼š**

```python
from pipeline import EvaluationPipeline
from tasks import DatasetRegistry
from evaluators import EvaluatorRegistry

# åˆå§‹åŒ–ç®¡çº¿
pipeline = EvaluationPipeline(
    model_path="/path/to/model",
    output_dir="./eval_results",
    seed=42
)

# è¿è¡Œè¯„ä¼°
results = pipeline.run(
    dataset_name="mmlu_pro",
    dataset_path="/path/to/mmlupro",
    evaluator_name="mmlu_pro",
    batch_size=64,
    max_length=10,
    limit=None,
    inferoutput="./mmlu_results.jsonl"  # ä¿å­˜æ¨ç†ç»“æœ
)

# è·å–æŒ‡æ ‡
print(f"Overall Accuracy: {results['metrics']['accuracy']:.2%}")
print(f"Correct: {results['metrics']['correct']}/{results['metrics']['total']}")

# æŒ‰å­¦ç§‘æŸ¥çœ‹
for subject, acc in results['metrics']['subject_accuracies'].items():
    print(f"{subject}: {acc:.2%}")
```

---

## â• æ·»åŠ æ–°æ•°æ®é›†

### æ­¥éª¤ 1: åˆ›å»ºæ³¨å†Œæ–‡ä»¶

åœ¨ `tasks/` ç›®å½•ä¸‹åˆ›å»º `register_your_dataset.py`ï¼š

```python
"""
æ³¨å†Œä½ çš„æ•°æ®é›†
"""

import json
from typing import List, Dict
from .dataset_registry import DatasetRegistry


# ==================== æ•°æ®åŠ è½½å™¨ ====================

def load_your_dataset(path: str) -> List[Dict]:
    """
    ä»æ–‡ä»¶åŠ è½½æ•°æ®
    
    è¿”å›æ ¼å¼ï¼šList[Dict]ï¼Œæ¯ä¸ª Dict åŒ…å«åŸå§‹æ•°æ®çš„æ‰€æœ‰å­—æ®µ
    """
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"  âœ“ åŠ è½½äº† {len(data)} æ¡æ•°æ®")
    return data


# ==================== Prompt Template ====================

def your_prompt_template(item: Dict) -> str:
    """
    å°†åŸå§‹æ•°æ®è½¬æ¢ä¸º prompt
    
    Args:
        item: åŸå§‹æ•°æ®çš„ä¸€æ¡è®°å½•
        
    Returns:
        æ ¼å¼åŒ–åçš„ prompt å­—ç¬¦ä¸²
    """
    question = item.get('question', '')
    context = item.get('context', '')
    
    if context:
        prompt = f"Context: {context}\n\nQuestion: {question}\n\nAnswer:"
    else:
        prompt = f"Question: {question}\n\nAnswer:"
    
    return prompt


# ==================== æ³¨å†Œ ====================

DatasetRegistry.register(
    name='your_dataset',
    loader=load_your_dataset,
    prompt_template=your_prompt_template,
    description='Your dataset description',
    default_batch_size=128,
    default_max_length=100
)
```

### æ­¥éª¤ 2: åœ¨ `__init__.py` ä¸­å¯¼å…¥

ç¼–è¾‘ `tasks/__init__.py`ï¼Œæ·»åŠ ï¼š

```python
try:
    from . import register_your_dataset
except ImportError as e:
    print(f"Warning: Could not import register_your_dataset: {e}")
```

### æ­¥éª¤ 3: ä½¿ç”¨æ–°æ•°æ®é›†

```bash
python3 run_pipeline.py \
    --dataset your_dataset \
    --dataset-path /path/to/your_data.json \
    --evaluator exact_match
```

---

## â• æ·»åŠ æ–°è¯„ä¼°å™¨

### æ­¥éª¤ 1: åˆ›å»ºæ³¨å†Œæ–‡ä»¶

åœ¨ `evaluators/` ç›®å½•ä¸‹åˆ›å»º `register_your_evaluator.py`ï¼š

```python
"""
æ³¨å†Œä½ çš„è¯„ä¼°å™¨
"""

from typing import List, Dict
from tqdm import tqdm
from .evaluator_registry import EvaluatorRegistry


def your_evaluator(
    engine,
    data: List[Dict],
    batch_size: int = 128,
    max_length: int = 100,
    **kwargs
) -> Dict[str, float]:
    """
    ä½ çš„è¯„ä¼°å™¨
    
    Args:
        engine: RWKVInferenceEngine å®ä¾‹
        data: æ•°æ®åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« 'prompt' å’Œ 'reference'
        batch_size: æ‰¹å¤„ç†å¤§å°
        max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
        **kwargs: å…¶ä»–å‚æ•°ï¼ˆå¦‚ inferoutputï¼‰
        
    Returns:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    print(f"\n=== Running Your Evaluator ===")
    print(f"Total samples: {len(data)}")
    print(f"Batch size: {batch_size}\n")
    
    correct = 0
    total = 0
    
    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
    pbar = tqdm(total=len(data), desc="Evaluating", unit="samples")
    
    for i in range(0, len(data), batch_size):
        batch = data[i:i+batch_size]
        prompts = [item['prompt'] for item in batch]
        
        # æ‰¹é‡æ¨ç†
        tokens, _ = engine.generate_batch(prompts, max_length=max_length)
        predictions = engine.decode_tokens(tokens)
        
        # è¯„ä¼°æ¯ä¸ªæ ·æœ¬
        for pred, item in zip(predictions, batch):
            reference = item['reference']
            
            # ä½ çš„è¯„ä¼°é€»è¾‘
            if pred.strip().lower() == reference.strip().lower():
                correct += 1
            total += 1
        
        # æ›´æ–°è¿›åº¦æ¡
        pbar.update(len(batch))
        pbar.set_postfix({'accuracy': f'{correct/total*100:.2f}%'})
    
    pbar.close()
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = correct / total if total > 0 else 0
    
    results = {
        'accuracy': accuracy,
        'correct': correct,
        'total': total
    }
    
    # æ‰“å°ç»“æœ
    print(f"\n=== Final Results ===")
    print(f"Accuracy: {accuracy*100:.2f}% ({correct}/{total})")
    
    return results


# ==================== æ³¨å†Œ ====================

EvaluatorRegistry.register(
    name='your_evaluator',
    evaluator=your_evaluator,
    description='Your evaluator description',
    metrics=['accuracy', 'correct', 'total']
)
```

### æ­¥éª¤ 2: åœ¨ `__init__.py` ä¸­å¯¼å…¥

ç¼–è¾‘ `evaluators/__init__.py`ï¼Œæ·»åŠ ï¼š

```python
try:
    from . import register_your_evaluator
except ImportError as e:
    print(f"Warning: Could not import register_your_evaluator: {e}")
```

### æ­¥éª¤ 3: ä½¿ç”¨æ–°è¯„ä¼°å™¨

```bash
python3 run_pipeline.py \
    --dataset your_dataset \
    --dataset-path /path/to/data.json \
    --evaluator your_evaluator
```

---

## ğŸ“¦ å†…ç½®ç»„ä»¶

### å†…ç½®æ•°æ®é›† (tasks/)

| åç§° | æè¿° | é»˜è®¤ batch_size | é»˜è®¤ max_length |
|------|------|----------------|----------------|
| `lambada` | LAMBADA è¯­è¨€å»ºæ¨¡ | 256 | 1 |
| `qa` | é€šç”¨é—®ç­” | 128 | 100 |
| `completion` | æ–‡æœ¬è¡¥å…¨ | 128 | 100 |
| `cot` | Chain-of-Thought æ¨ç† | 64 | 200 |
| `math` | æ•°å­¦é—®é¢˜ | 64 | 150 |
| `mmlu_pro` | MMLU Pro å¤šé€‰é¢˜ | 64 | 10 |
| `mmlu_pro_cot` | MMLU Pro (CoTç‰ˆæœ¬) | 32 | 200 |

### å†…ç½®è¯„ä¼°å™¨ (evaluators/)

| åç§° | æè¿° | æ”¯æŒçš„æŒ‡æ ‡ |
|------|------|-----------|
| `perplexity` | å›°æƒ‘åº¦å’Œå‡†ç¡®ç‡ | `perplexity`, `accuracy`, `correct`, `total` |
| `exact_match` | ç²¾ç¡®åŒ¹é… | `exact_match`, `correct`, `total` |
| `generation` | ç”Ÿæˆç»Ÿè®¡ | `avg_length`, `total_tokens` |
| `mmlu_pro` | MMLU Pro å¤šé€‰é¢˜è¯„ä¼° | `accuracy`, `correct`, `total`, `subject_accuracies`, `num_subjects` |

---

## ğŸ’» Python API

### DatasetRegistry API

```python
from tasks import DatasetRegistry

# åˆ—å‡ºæ‰€æœ‰æ•°æ®é›†
datasets = DatasetRegistry.list_datasets()
print(datasets)  # ['lambada', 'mmlu_pro', ...]

# è·å–æ•°æ®é›†é…ç½®
config = DatasetRegistry.get('mmlu_pro')
print(config.description)
print(config.default_batch_size)

# åŠ è½½æ•°æ®é›†
data = DatasetRegistry.load_dataset(
    name='mmlu_pro',
    path='/path/to/mmlupro',
    limit=100  # å¯é€‰ï¼šé™åˆ¶æ ·æœ¬æ•°
)

# æ¯æ¡æ•°æ®åŒ…å«ï¼š
# - 'prompt': æ ¼å¼åŒ–åçš„ prompt
# - 'reference': å‚è€ƒç­”æ¡ˆ
# - 'raw': åŸå§‹æ•°æ®
for item in data[:3]:
    print(item['prompt'])
    print(item['reference'])
```

### EvaluatorRegistry API

```python
from evaluators import EvaluatorRegistry

# åˆ—å‡ºæ‰€æœ‰è¯„ä¼°å™¨
evaluators = EvaluatorRegistry.list_evaluators()
print(evaluators)  # ['perplexity', 'mmlu_pro', ...]

# è·å–è¯„ä¼°å™¨é…ç½®
config = EvaluatorRegistry.get('mmlu_pro')
print(config.description)
print(config.metrics)

# è¿è¡Œè¯„ä¼°
from batch_engine import RWKVInferenceEngine

engine = RWKVInferenceEngine(model_path="/path/to/model")
data = [...]  # åŠ è½½çš„æ•°æ®

results = EvaluatorRegistry.evaluate(
    name='mmlu_pro',
    engine=engine,
    data=data,
    batch_size=64,
    inferoutput='./results.jsonl'  # å¯é€‰
)

print(results)
# {'accuracy': 0.35, 'correct': 350, 'total': 1000, ...}
```

### RWKVInferenceEngine API

```python
from batch_engine import RWKVInferenceEngine

# åˆå§‹åŒ–å¼•æ“
engine = RWKVInferenceEngine(
    model_path="/path/to/model.pth",
    vocab_path=None,  # å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æ ‡å‡†è¯æ±‡è¡¨
    vocab_size=65536,
    head_size=64,
    seed=42
)

# æ‰¹é‡ç”Ÿæˆ
prompts = ["Question 1", "Question 2", "Question 3"]
tokens, inference_time = engine.generate_batch(
    prompts=prompts,
    max_length=100,
    noise=0.0
)

# è§£ç 
texts = engine.decode_tokens(tokens)
for text in texts:
    print(text)

# ç”Ÿæˆå¹¶è¿”å› log æ¦‚ç‡ï¼ˆç”¨äº logprobs ä»»åŠ¡ï¼‰
logprobs_result = engine.generate_with_logprobs(
    prompt="Question",
    max_length=10,
    top_logprobs=5,
    echo=True
)
```

---

## ğŸ“Š è¾“å‡ºæ ¼å¼

### 1. è¯„ä¼°æŒ‡æ ‡æ–‡ä»¶ (JSON)

ä¿å­˜åœ¨ `./eval_results/` ç›®å½•ï¼š

```json
{
  "timestamp": "2024-10-16 12:34:56",
  "model_path": "/path/to/model",
  "dataset": "mmlu_pro",
  "evaluator": "mmlu_pro",
  "config": {
    "batch_size": 64,
    "max_length": 10,
    "seed": 42
  },
  "metrics": {
    "accuracy": 0.3542,
    "correct": 4263,
    "total": 12032,
    "num_subjects": 14,
    "subject_accuracies": {
      "Math": 0.42,
      "Physics": 0.38,
      "Biology": 0.35,
      ...
    }
  },
  "inference_time": 1234.56
}
```

### 2. æ¨ç†ç»“æœæ–‡ä»¶ (JSONL)

å¦‚æœæŒ‡å®šäº† `--inferoutput`ï¼Œä¿å­˜æ¯æ¡æ ·æœ¬çš„æ¨ç†è¯¦æƒ…ï¼š

```jsonl
{"question": "What is...", "options": ["A...", "B...", ...], "category": "Math", "prompt": "Question: ...", "prediction": "A", "predicted_answer": "A", "correct_answer": "B", "is_correct": false}
{"question": "Which...", "options": ["A...", "B...", ...], "category": "Physics", "prompt": "Question: ...", "prediction": "C", "predicted_answer": "C", "correct_answer": "C", "is_correct": true}
...
```

æ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ï¼š
- `question`: åŸå§‹é—®é¢˜
- `options`: é€‰é¡¹åˆ—è¡¨
- `category`: å­¦ç§‘/ç±»åˆ«
- `prompt`: å®Œæ•´çš„è¾“å…¥ prompt
- `prediction`: æ¨¡å‹çš„åŸå§‹è¾“å‡º
- `predicted_answer`: æå–çš„ç­”æ¡ˆ
- `correct_answer`: æ­£ç¡®ç­”æ¡ˆ
- `is_correct`: æ˜¯å¦æ­£ç¡®

---

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•è°ƒæ•´æ‰¹å¤„ç†å¤§å°ä»¥é€‚åº”æ˜¾å­˜ï¼Ÿ

A: æ ¹æ®ä½ çš„ GPU æ˜¾å­˜è°ƒæ•´ `--batch-size`ï¼š

```bash
# æ˜¾å­˜å……è¶³ (24GB+)
--batch-size 128

# æ˜¾å­˜ä¸­ç­‰ (12GB)
--batch-size 64

# æ˜¾å­˜è¾ƒå° (8GB)
--batch-size 32
```

### Q2: å¦‚ä½•åªæµ‹è¯•éƒ¨åˆ†æ•°æ®è¿›è¡Œå¿«é€ŸéªŒè¯ï¼Ÿ

A: ä½¿ç”¨ `--limit` å‚æ•°ï¼š

```bash
python3 run_pipeline.py \
    --dataset mmlu_pro \
    --dataset-path /path/to/data \
    --evaluator mmlu_pro \
    --limit 100  # åªæµ‹è¯•å‰100ä¸ªæ ·æœ¬
```

### Q3: æ¨ç†ç»“æœä¿å­˜åœ¨å“ªé‡Œï¼Ÿ

A: 
- **è¯„ä¼°æŒ‡æ ‡**: `./eval_results/<dataset>_<evaluator>_<timestamp>.json`
- **æ¨ç†è¯¦æƒ…** (å¦‚æœæŒ‡å®š `--inferoutput`): ä½ æŒ‡å®šçš„è·¯å¾„

### Q4: å¦‚ä½•æŸ¥çœ‹å®æ—¶è¿›åº¦ï¼Ÿ

A: è¯„ä¼°è¿‡ç¨‹ä¼šæ˜¾ç¤º tqdm è¿›åº¦æ¡ï¼š

```
Evaluating MMLU-Pro:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 5400/12032 [02:15<02:45, 40.1samples/s, accuracy=28.45%]
```

### Q5: MMLU-Pro ä½¿ç”¨å“ªç§è¯„ä¼°æ–¹æ³•ï¼Ÿ

A: å½“å‰ä½¿ç”¨**ç”Ÿæˆå¼æ–¹æ³•**ï¼ˆå®˜æ–¹æ ‡å‡†ï¼‰ï¼š
1. æ¨¡å‹ç”Ÿæˆè¾“å‡ºï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªå­—æ¯ A-Jï¼‰
2. ä»è¾“å‡ºä¸­æå–ç­”æ¡ˆå­—æ¯
3. ä¸æ­£ç¡®ç­”æ¡ˆæ¯”è¾ƒ

è¿™ä¸ MMLU-Pro å®˜æ–¹è¯„ä¼°æ–¹æ³•ä¸€è‡´ã€‚

### Q6: å¦‚ä½•ä½¿ç”¨ Chain-of-Thought è¯„ä¼°ï¼Ÿ

A: å¯¹äº MMLU-Proï¼Œä½¿ç”¨ `--cot` å‚æ•°ï¼š

```bash
python3 run_mmlu_pro.py \
    --dataset-path /path/to/mmlupro \
    --cot \
    --max-length 200  # CoT éœ€è¦æ›´é•¿çš„ç”Ÿæˆé•¿åº¦
```

### Q7: æ•°æ®é›†å’Œ API è¯„ä¼°æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

| ç‰¹æ€§ | æœ¬åœ°æ‰¹é‡è¯„ä¼° | API è¯„ä¼° |
|------|------------|---------|
| ä½ç½® | `local_batch_benchmark/` | `api_benchmark/` |
| è¿è¡Œæ–¹å¼ | ç›´æ¥è°ƒç”¨æ¨ç†å¼•æ“ | é€šè¿‡ HTTP API |
| é€‚ç”¨åœºæ™¯ | æœ¬åœ°æµ‹è¯•ã€å¿«é€Ÿè¿­ä»£ | ä¸ lm_eval ç­‰å¤–éƒ¨å·¥å…·é›†æˆ |
| æ€§èƒ½ | æ›´å¿«ï¼ˆæ— ç½‘ç»œå¼€é”€ï¼‰ | ç¨æ…¢ï¼ˆæœ‰ HTTP å¼€é”€ï¼‰ |
| çµæ´»æ€§ | å®Œå…¨æ§åˆ¶ | æ ‡å‡†åŒ–æ¥å£ |

### Q8: å¦‚ä½•æ·»åŠ å¯¹æ–°æ•°æ®é›†çš„æ”¯æŒï¼Ÿ

A: å‚è§ [æ·»åŠ æ–°æ•°æ®é›†](#-æ·»åŠ æ–°æ•°æ®é›†) ç« èŠ‚ã€‚

### Q9: è¯„ä¼°å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

A: ä¼˜åŒ–å»ºè®®ï¼š
1. å¢åŠ  `--batch-size`ï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
2. å‡å°‘ `--max-length`ï¼ˆå¦‚æœä¸éœ€è¦é•¿è¾“å‡ºï¼‰
3. ä½¿ç”¨ `--limit` å¿«é€Ÿæµ‹è¯•
4. ç¡®ä¿ä½¿ç”¨ GPU æ¨ç†

### Q10: å¦‚ä½•è‡ªå®šä¹‰ prompt æ¨¡æ¿ï¼Ÿ

A: ä¿®æ”¹å¯¹åº”æ•°æ®é›†çš„ `register_*.py` æ–‡ä»¶ä¸­çš„ prompt æ¨¡æ¿å‡½æ•°ï¼Œæˆ–åˆ›å»ºæ–°çš„æ•°æ®é›†æ³¨å†Œã€‚

---

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ï¼š
- **ç¤ºä¾‹ä»£ç **: `example_usage.py`
- **æµ‹è¯•è„šæœ¬**: `run_pipeline.py --example 1`
- **å†…è”æ–‡æ¡£**: å„æ¨¡å—çš„ docstring

---

## ğŸ“ è®¸å¯

æœ¬é¡¹ç›®éµå¾ª Apache 2.0 è®¸å¯è¯ã€‚

---

**Happy Evaluating! ğŸš€**
